{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基本概念\n",
    "    - tf.logging\n",
    "    - tf.summary\n",
    "    - tf.Print\n",
    "    - tf.estimator\n",
    "    - tf.data\n",
    "    - tf.nn\n",
    "    - TFRecord\n",
    "    - tf中的Feature\n",
    "    - sess\n",
    "    - 数据类型 \n",
    "    - 激活函数\n",
    "    - 损失函数\n",
    "    - 优化器\n",
    "    - 指标\n",
    "    - 异常\n",
    "- 变量\n",
    "- 常见张量计算\n",
    "- 常见张量操作\n",
    "- 基本流程\n",
    "- 构建网络\n",
    "    - scope\n",
    "    - 嵌入层及其他网络层\n",
    "    - 其他操作: 归一化\n",
    "- Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本概念"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# tensorflow1.0: 静态图\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:...\n"
     ]
    }
   ],
   "source": [
    "# tf.logging 本身就调用了 logging ，会产生两套 handlers，使得每次tf.logging.info(\"...\") 时会将其中内容打印两遍\n",
    "tf.logging.info(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.logging\n",
    "    # set_verbosity(tf.compat.v1.logging.INFO)\n",
    "    # 除了INFO(20)，还有DEBUG(10)，WARN(30)，ERROR(40)和FATAL(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.summary: 能够保存训练过程以及参数分布图并在tensorboard显示\n",
    "    # tf.summary.scalar(tags, values, collections=None, name=None): 用来显示标量信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.Print(input, data, message=None, summarize=None, name=None)\n",
    "    # 最低要求两个输入，input和data，input是需要打印的变量的名字，data要求是一个list，里面包含要打印的内容\n",
    "    # message是需要输出的错误信息\n",
    "    # summarize是对每个tensor只打印的条目数量，如果是None，对于每个输入tensor只打印3个元素\n",
    "    # name是op的名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "x=tf.constant([2,3,4,5])\n",
    "x=tf.Print(x,[x,x.shape,'test', x],message='Debug message:',summarize=2)\n",
    "with tf.Session() as sess:\n",
    "    print(x.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.estimator\n",
    "    # 如果使用Estimator，就不用再使用Session\n",
    "    # tf.estimator.RunConfig: 用于控制内部和checkpoints等\n",
    "        # model_dir: 指定存储模型参数，graph等的路径\n",
    "        # save_summary_steps: 每隔多少step就存一次Summaries\n",
    "        # save_checkpoints_steps:每隔多少个step就存一次checkpoint\n",
    "        # save_checkpoints_secs: 每隔多少秒就存一次checkpoint，不可以和save_checkpoints_steps同时指定。如果二者都不指定，则使用默认值，即每600秒存一次。如果二者都设置为None，则不存checkpoints\n",
    "        # keep_checkpoint_max：指定最多保留多少个checkpoints，也就是说当超出指定数量后会将旧的checkpoint删除。当设置为None或0时，则保留所有checkpoints\n",
    "        # log_step_count_steps:该参数的作用是,(相对于总的step数而言)指定每隔多少step就记录一次训练过程中loss的值，同时也会记录global steps/s，通过这个也可以得到模型训练的速度快慢\n",
    "    # tf.estimator.Estimator(model_fn, config, params, model_dir)，\n",
    "        # model_fn为自定义的网络模型函数\n",
    "        # config用于控制内部和checkpoints等\n",
    "        # params该参数的值会传递给model_fn\n",
    "        # model_dir指定checkpoints和其他日志存放的路径\n",
    "        # estimator.predict\n",
    "        # 定义在miniconda3/envs/ctrtf/lib/python3.6/site-packages/tensorflow_estimator/python/estimator下的estimator.py文件中\n",
    "    # tf.estimator.experimental.stop_if_no_decrease_hook:\n",
    "        # 定义early_stopping\n",
    "        # 参数:\n",
    "            # estimator: 模型\n",
    "            # metric_name: 需要追踪的评估指标名称\n",
    "            # max_steps_without_increase: 指定评估指标没有提升的情况下最大运行步数\n",
    "    # tf.estimator.TrainSpec(input_fn, max_steps=None, hooks=None)\n",
    "        # input_fn: 用来指定数据输入\n",
    "        # max_steps: 用来指定训练的最大步数\n",
    "        # hooks: 用来在 session 运行的时候做一些额外的操作\n",
    "        # 定义在miniconda3/envs/ctrtf/lib/python3.6/site-packages/tensorflow_estimator/python/estimator下的training.py文件中\n",
    "    # tf.estimator.EvalSpec(input_fn, steps, throttle_secs)\n",
    "        # steps: 用来指定评估的迭代步数，如果为None，则在整个数据集上评估\n",
    "        # throttle_secs: 多少秒后又开始评估，如果没有新的 checkpoints 产生，则不评估，所以这个间隔是最小值\n",
    "        # 定义在miniconda3/envs/ctrtf/lib/python3.6/site-packages/tensorflow_estimator/python/estimator下的training.py文件中\n",
    "    # tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "        # 定义在miniconda3/envs/ctrtf/lib/python3.6/site-packages/tensorflow_estimator/python/estimator下的training.py文件中\n",
    "    # tf.estimator.EstimatorSpec\n",
    "        # mode: ModeKeys\n",
    "        # predictions\n",
    "        # loss: 训练损失Tensor\n",
    "        # train_op\n",
    "        # eval_metric_ops\n",
    "        # 不同模式需要传入不同参数，对于mode == ModeKeys.TRAIN:必填字段是loss和train_op；对于mode == ModeKeys.EVAL:必填字段是loss；对于mode == ModeKeys.PREDICT:必填字段是predictions\n",
    "    # tf.estimator.ModeKeys.PREDICT\n",
    "    # tf.estimator.ModeKeys.EVAL\n",
    "    # tf.estimator.ModeKeys.PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data\n",
    "    # tf.data.TextLineDataset\n",
    "        # 可处理csv和libsvm文件\n",
    "    # tf.data.TFRecordDataset\n",
    "        # 可处理tfrecord文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.nn\n",
    "    # tf.nn.bias_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.python_io.TFRecordWriter(path)-->tf.io.TFRecordWriter\n",
    "    # 为什么使用TFRecord?: TFRecord格式的文件存储形式会很合理的帮我们存储数据。TFRecord内部使用了“Protocol Buffer”二进制数据编码方案，它只占用一个内存块，只需要一次性加载一个二进制文件的方式即可，简单，快速，尤其对大型训练数据很友好。而且当我们的训练数据量比较大的时候，可以将数据分成多个TFRecord文件，来提高处理效率\n",
    "    # writer.write(example.SerializeToString())，其中example.SerializeToString()是将example中的map压缩为二进制文件，更好的节省空间\n",
    "    # example的构建: tf.train.Example(features = None)+tf.train.Features(feature = None)，其中feature是个map，key是要保存数据的名字，value是要保存的数据，格式必须符合tf.train.Feature实例要求\n",
    "    # writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.FixedLenFeature和tf.VarLenFeature-->tf.io.FixedLenFeature和tf.io.VarLenFeature\n",
    "    # 前者返回一个定长的tensor，后者返回不定长的；前者可传入shape参数，如为[]则要求tensor的shape=(batch_size,)，若为[k]，则输出tensor的shape=(batch_size,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.feature_column和tfrecord搭配使用，特征名要一致\n",
    "# tf.feature_column.categorical_column_with_identity(): \n",
    "    # 参数有特征名、num_buckets和default_value\n",
    "    # 得到one-hot编码，即输出为N*num_buckets的矩阵\n",
    "# tf.feature_column.embedding_column\n",
    "    # 参数有categorical_column、dimension\n",
    "    # 将sparse/categrical特征转化为dense向量\n",
    "# tf.feature_column.numeric_column\n",
    "    # 参数有特征名\n",
    "    # 用于抽取数值类型的特征，即dense特征\n",
    "# tf.feature_column.input_layer(features, feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess\n",
    "# sess.run()这个里面只能放operation和tensor，还有参数feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(1)\n",
    "sess = tf.InteractiveSession()\n",
    "print(x.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype\n",
    "    # tf.int64\n",
    "    # tf.float32\n",
    "    # tf.bool\n",
    "# tf.to_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 激活函数\n",
    "    # tf.nn.softmax\n",
    "    # tf.nn.sigmoid\n",
    "    # tf.nn.relu\n",
    "\n",
    "    # tf.sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "    # tf.nn.sigmoid_cross_entropy_with_logits\n",
    "\n",
    "# tf.keras.backend.binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器\n",
    "    # tf.train.AdagradOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指标\n",
    "    # tf.metrics.accuracy\n",
    "    # tf.metrics.auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 异常\n",
    "    # InvalidArgumentError: 当操作接收到无效参数时触发.例如,如果某个操作接收到的输入张量具有无效的值或形状,则可能发生该情况."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.constant(list) | tf.constant(list, shape=[2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_2:0\", shape=(3,), dtype=int32)\n",
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1,2,3])\n",
    "print(a)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.identity(input_tensor)：返回一个和input_tensor形状、内容都相同的tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "b = tf.identity(a)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.ones_like | tf.zeros_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.one_hot(indices, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random_normal(shape=[2,3], mean=0, stddev=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不能直接将tf.Tensor作为bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.SparseTensor(values,indices,dense_shape)：三个参数均可使用np.array定义，定义稀疏tensor，可以节省内存\n",
    "# tf.sparse_tensor_to_dense(sp_input,default_value=0)：将 SparseTensor 转换为稠密张量\n",
    "# tf.sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.Variable()和tf.get_variable()有不同的创建变量的方式：\n",
    "    # tf.Variable() 每次都会新建变量。如果希望重用（共享）一些变量，就需要用到了get_variable()，它会去搜索变量名，有就直接用，没有再新建\n",
    "# tf.Variable(initial_value=0, name=\"...\", trainable=False, dtype=tf.int64, collections=[...])\n",
    "# tf.get_variable(name=\"...\", shape, collections=[...], initializer=tf.truncated_normal_initializer(stddev=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.contrib.framework.python.ops import variables as contrib_variables\n",
    "# contrib_variables.model_variable('...', shape, initializer=tf.zeros_initializer(), trainable)\n",
    "\n",
    "# from tensorflow.python.ops import nn_ops\n",
    "# nn_ops.bias_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.get_collection(key)：可以用来获取key集合中的所有变量，返回一个列表。如GraphKeys类包含许多标准集合名，如tf.GraphKeys.REGULARIZATION_LOSSES、tf.GraphKeys.VARIABLES、tf.GraphKeys.TRAINABLE_VARIABLES、tf.GraphKeys.GLOBAL_STEP、tf.GraphKeys.GLOBAL_VARIABLES、tf.GraphKeys.UPDATE_OPS\n",
    "\n",
    "# tf.add_to_collection('all_loss', self.loss)\n",
    "# tf.get_collection('all_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常见张量计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reduce_sum | tf.reduce_mean | tf.reduce_max\n",
    "    # 如果不指定维度，则计算所有元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.pow | tf.square | tf.add | tf.sqrt | tf.exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乘法\n",
    "    # tf.multiply(x, y)：两个矩阵中对应元素各自相乘\n",
    "    # tf.matmul(a, b)：将矩阵a乘以矩阵b\n",
    "    # tf.tensordot(a, b, axes)\n",
    "        # 在axes=2轴上相乘指的是将a的最后两个维度与b的前两个维度矩阵相乘，然后将结果累加求和，消除(收缩)这四个维度，矩阵a，b剩下的维度concat，就是所求矩阵维度\n",
    "        # 在axes=[[1,3],[0,2]]上进行tensor相乘，指的是将a的第一个维度、第三个维度concat的维度与b的第0(维度下标从0开始)个维度、第二个维度concat的维度进行矩阵相乘，然后将结果累加求和，消除(收缩)这四个维度，矩阵a，b剩下的维度concat，就是所求矩阵维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a的shape: (2, 1, 3, 2)\n",
      "b的shape: (2, 3, 1)\n",
      "res1 shape: (2, 1, 1)\n",
      "res2 shape: (2, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([0,1,2,1,3,4,5,2,3,4,5,0],shape=[2,1,3,2])\n",
    "b =tf.constant([1,3,2,3,1,2],shape=[2,3,1])\n",
    "res1 = tf.tensordot(a,b,axes=2)\n",
    "res2 = tf.tensordot(a,b,axes=[[1,3],[0,2]])\n",
    "print(\"a的shape:\",a.shape)\n",
    "print(\"b的shape:\",b.shape)\n",
    "print(\"res1 shape:\",res1.shape)\n",
    "print(\"res2 shape:\",res2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.logical_or(x, y)：期望输入布尔类型。输入类型为张量，如果张量包含多个元素，则将按元素进行逻辑或运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.3102722  -0.9942122  -1.876304  ]\n",
      " [-0.16043976 -0.2800879   0.8441572 ]]\n",
      "[array([ 0.57491624, -0.63715005, -0.5160734 ], dtype=float32), array([0.5407484 , 0.12749338, 1.8502275 ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# 计算均值和方差\n",
    "img = tf.Variable(tf.random_normal([2, 3]))\n",
    "mean, variance = tf.nn.moments(img, axes=0)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(img))\n",
    "    print(sess.run([mean, variance]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常见张量操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor.get_shape().as_list()\n",
    "# tf.shape：获取张量的形状；返回的是一个tensor\n",
    "# tensorflow打印张量shape，如遇自定义变量(batch size)则会显示None\n",
    "# tf.reshape(tensor, shape)：若shape为[]，输出标量；若shape为[-1]，则展平张量\n",
    "# tf.expand_dims(input,axis=None)：在axis轴处给input增加一个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.concat(values, axis)：按照指定的已经存在的轴进行拼接\n",
    "# tf.stack(values, axis)：按照指定的新建的轴进行拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.tile(input, multiples)：两个参数形状需要一致，第二个参数指明每个维度的复制次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.split(input_tensor,num_or_size_splits,axis=0)：num_or_size_splits如果是个整数n，就将输入的tensor分为n个子tensor。如果是个list T，就将输入的tensor分为len(T)个子tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[array([[-0.3797491 ,  0.39872998, -0.23381083]], dtype=float32), array([[-1.3213638 , -0.13312116, -1.4646379 ]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "c = tf.random_normal([2,3])\n",
    "cs = tf.split(c, 2, axis=0)\n",
    "print(type(cs))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(cs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.argmax | tf.maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.equal(A, B)：对比这两个矩阵或者向量的相等的元素，如果是相等的那就返回True，否则返回False，返回的值的矩阵维度和A是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.where(condition, x=None, y=None)：若只有参数condition，则返回condition中为True的索引。若有x和y，则x, y相同维度，返回值是对应元素，condition中元素为True的元素替换为x中的元素，为False的元素替换为y中对应元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.sequence_mask(lengths,maxlen=None,dtype=tf.bool)：用于数据填充。返回值mask张量：默认mask张量就是布尔格式的一种张量表达，只有True和 False 格式，也可以通过参数dtype指定其他数据格式。参数lengths：表示的是长度；可以是标量，也可以是列表 [ ] ，也可以是二维列表[ [ ],[ ] ,…]，甚至是多维列表…，一般列表类型的用的比较多。参数maxlen：当默认None，默认从lengths中获取最大的那个数字，决定返回mask张量的长度；当为N时，返回的是N长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'SequenceMask/Less:0' shape=(3, 5) dtype=bool>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sequence_mask(tf.constant([2,3,4]), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.assign(A, new_number)：这个函数的功能主要是把A的值变为new_number\n",
    "# tf.assign_add(ref,value)：更新ref的值，通过增加value，即：ref = ref + value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.cond()类似于if...else..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = tf.Variable(tf.zeros([100])) # 100维向量\n",
    "# W = tf.Variable(tf.random_uniform([784, 100], -1, 1))\n",
    "# x = tf.placeholder(dtype=tf.float32, name=\"x\")\n",
    "# relu = tf.nn.relu(tf.matmul(W, x)+b)\n",
    "# C = [...] # Cost\n",
    "# s = tf.Session()\n",
    "# for step in range(0, 10):\n",
    "#     input = 100维向量\n",
    "#     result = s.run(C, feed_dict={x:input})\n",
    "#     print(step, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow变量作用域\n",
    "    # tf.name_scope()、tf.variable_scope()是两个作用域函数，一般与两个创建/调用变量的函数tf.Variable() 和tf.get_variable()搭配使用。常用于：1）变量共享；2）tensorboard画流程图进行可视化封装变量\n",
    "    # tf.name_scope()/命名域、tf.variable_scope()/变量域会在模型中开辟各自的空间，而其中的变量均在这个空间内进行管理\n",
    "        # 命名域还可由tf.op_scope创建，变量域还可由tf.variable_op_scope创建\n",
    "        # 对于上述两种作用域，对于使用tf.Variable()方式创建的变量，具有相同的效果，都会在变量名称前面，加上域名称。对于通过tf.get_variable()方式创建的变量，只有variable scope名称会加到变量名称前面，而name scope不会作为前缀\n",
    "    # variable_scope可以通过设置reuse标志以及初始化方式来影响域下的变量，因为想要达到变量共享的效果, 就要在 tf.variable_scope()的作用域下使用 tf.get_variable() 这种方式产生和提取变量. 不像 tf.Variable() 每次都会产生新的变量, tf.get_variable() 如果遇到了已经存在名字的变量时, 它会单纯的提取这个同样名字的变量，如果不存在名字的变量再创建\n",
    "    # 必须要在tf.variable_scope的作用域下使用tf.get_variable()函数。这里用tf.get_variable( ) 而不用tf.Variable( )，是因为前者拥有一个变量检查机制，会检测已经存在的变量是否设置为共享变量，如果已经存在的变量没有设置为共享变量，TensorFlow 运行到第二个拥有相同名字的变量的时候，就会报错\n",
    "    # with tf.variable_scope(name_or_scope=\"...\", reuse=tf.AUTO_REUSE) as scope:\n",
    "    # with tf.name_scope(\"...\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1:0\n",
      "my_name_scope/var2:0\n",
      "my_name_scope/Add:0\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"my_name_scope\"):\n",
    "    v1 = tf.get_variable(\"var1\", [1], dtype=tf.float32) \n",
    "    v2 = tf.Variable(1, name=\"var2\", dtype=tf.float32)\n",
    "    a = tf.add(v1, v2)\n",
    "    print(v1.name)\n",
    "    print(v2.name) \n",
    "    print(a.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_variable_scope/var1:0\n",
      "my_variable_scope/var2:0\n",
      "my_variable_scope/Add:0\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"my_variable_scope\"):\n",
    "    v1 = tf.get_variable(\"var1\", [1], dtype=tf.float32)\n",
    "    v2 = tf.Variable(1, name=\"var2\", dtype=tf.float32)\n",
    "    a = tf.add(v1, v2)\n",
    "    print(v1.name) \n",
    "    print(v2.name)\n",
    "    print(a.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import variable_scope\n",
    "# with variable_scope.variable_scope(\"name\",reuse=tf.AUTO_REUSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.framework.python.ops import arg_scope\n",
    "# with arg_scope(): 指定网络的默认参数；第一个参数如[layers.conv2d]表示要执行操作的网络，后续的参数就是要设定默认的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 嵌入\n",
    "# tf.nn.embedding_lookup(嵌入矩阵, 索引)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.contrib import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers.input_from_feature_columns(columns_to_tensors,feature_columns,scope=tf.variable_scope)\n",
    "# layers.fully_connected(输入, 输出维度, activation_fn=tf.nn.softmax, variables_collections, outputs_collections, scope)\n",
    "# layers.linear(输入, 输出维度, scope, variables_collections, outputs_collections)\n",
    "# layers.batch_norm(input_vec, scale=True, is_training, scope, variables_collections, outputs_collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.layers.dense\n",
    "# tf.layers.batch_normalization\n",
    "# tf.layers.dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归一化\n",
    "# tf.nn.l2_normalize(张量)：元素除以各自的范数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.keras import backend\n",
    "# from tensorflow.python.keras.initializers import RandomNormal, Zeros, glorot_normal\n",
    "# from tensorflow.python.keras.layers import Input, Lambda, Embedding, Layer, Flatten\n",
    "    # Input中参数shape不包括batch size，例如shape=(32,) 表示了预期的输入将是一批32维的向量\n",
    "    # Layer\n",
    "        # bulid()\n",
    "        # call()\n",
    "# from tensorflow.python.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.layers.Layer\n",
    "# tf.keras.layers.Concatenate\n",
    "# tf.keras.layers.add\n",
    "# tf.keras.layers.Dense\n",
    "# tf.keras.models.Model(inputs, outputs)\n",
    "    # 首先定义好网络，再将网络的输入和输出部分作为参数定义一个Model类对象\n",
    "    # model.compile()\n",
    "    # model.fit()\n",
    "        # verbose: 日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录，2为每个epoch输出一行记录\n",
    "    # model.predict()\n",
    "# tf.keras.initializers.glorot_normal"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f731b860690565808261aa16d59807e4733507b408e88bbb2c86737b8c69cb28"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('tfone': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}