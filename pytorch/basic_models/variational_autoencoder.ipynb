{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0cc86fbc300e6d473e5cf532321586f93c8d337e756c8fe34a22a2b65e82a1ba0",
   "display_name": "Python 3.7.9 64-bit ('piprnn': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "变分自动编码器: MNIST图像生成\n",
    "    只使用训练数据\n",
    "    定义编码器和解码器，均由线性层构成\n",
    "    编码器产生均值和方差进行重参数得到隐含变量，之后利用解码器进行重建\n",
    "    使用二元交叉熵损失和KL散度作为最终损失\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory if not exists\n",
    "sample_dir = 'vae_samples'\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "image_size = 784\n",
    "h_dim = 400\n",
    "z_dim = 20\n",
    "num_epochs = 15\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "dataset = torchvision.datasets.MNIST(root='../data',\n",
    "                                     train=True,\n",
    "                                     transform=transforms.ToTensor(),\n",
    "                                     download=True)\n",
    "\n",
    "# Data loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=784, h_dim=400, z_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(image_size, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim)\n",
    "        self.fc5 = nn.Linear(h_dim, image_size)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        return self.fc2(h), self.fc3(h)\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        return F.sigmoid(self.fc5(h))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconst = self.decode(z)\n",
    "        return x_reconst, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/469], Reconst Loss: 10563.4980, KL Div: 3271.4248\n",
      "Epoch[10/15], Step [60/469], Reconst Loss: 10674.3652, KL Div: 3283.5684\n",
      "Epoch[10/15], Step [70/469], Reconst Loss: 9938.0234, KL Div: 3203.2808\n",
      "Epoch[10/15], Step [80/469], Reconst Loss: 10683.5938, KL Div: 3359.8535\n",
      "Epoch[10/15], Step [90/469], Reconst Loss: 10355.2607, KL Div: 3187.9998\n",
      "Epoch[10/15], Step [100/469], Reconst Loss: 10113.1152, KL Div: 3170.6177\n",
      "Epoch[10/15], Step [110/469], Reconst Loss: 9958.2070, KL Div: 3176.1445\n",
      "Epoch[10/15], Step [120/469], Reconst Loss: 10205.7861, KL Div: 3155.8237\n",
      "Epoch[10/15], Step [130/469], Reconst Loss: 10275.2510, KL Div: 3254.6748\n",
      "Epoch[10/15], Step [140/469], Reconst Loss: 10442.8516, KL Div: 3196.7402\n",
      "Epoch[10/15], Step [150/469], Reconst Loss: 10477.3164, KL Div: 3275.6045\n",
      "Epoch[10/15], Step [160/469], Reconst Loss: 10687.8506, KL Div: 3218.0181\n",
      "Epoch[10/15], Step [170/469], Reconst Loss: 10518.9844, KL Div: 3302.6875\n",
      "Epoch[10/15], Step [180/469], Reconst Loss: 10230.9434, KL Div: 3239.7344\n",
      "Epoch[10/15], Step [190/469], Reconst Loss: 10251.2246, KL Div: 3357.3755\n",
      "Epoch[10/15], Step [200/469], Reconst Loss: 10339.6992, KL Div: 3194.6628\n",
      "Epoch[10/15], Step [210/469], Reconst Loss: 11120.5488, KL Div: 3285.8062\n",
      "Epoch[10/15], Step [220/469], Reconst Loss: 10477.1738, KL Div: 3260.5720\n",
      "Epoch[10/15], Step [230/469], Reconst Loss: 10417.0566, KL Div: 3289.8809\n",
      "Epoch[10/15], Step [240/469], Reconst Loss: 10047.2949, KL Div: 3198.7832\n",
      "Epoch[10/15], Step [250/469], Reconst Loss: 10342.0811, KL Div: 3315.2104\n",
      "Epoch[10/15], Step [260/469], Reconst Loss: 9983.1172, KL Div: 3231.7529\n",
      "Epoch[10/15], Step [270/469], Reconst Loss: 10338.2188, KL Div: 3292.1572\n",
      "Epoch[10/15], Step [280/469], Reconst Loss: 10424.6387, KL Div: 3255.3687\n",
      "Epoch[10/15], Step [290/469], Reconst Loss: 10630.4941, KL Div: 3226.5447\n",
      "Epoch[10/15], Step [300/469], Reconst Loss: 10177.8145, KL Div: 3261.0769\n",
      "Epoch[10/15], Step [310/469], Reconst Loss: 10693.9590, KL Div: 3263.2378\n",
      "Epoch[10/15], Step [320/469], Reconst Loss: 9753.5352, KL Div: 3185.2341\n",
      "Epoch[10/15], Step [330/469], Reconst Loss: 10316.2188, KL Div: 3279.6626\n",
      "Epoch[10/15], Step [340/469], Reconst Loss: 10135.9004, KL Div: 3239.2078\n",
      "Epoch[10/15], Step [350/469], Reconst Loss: 10290.8926, KL Div: 3199.0146\n",
      "Epoch[10/15], Step [360/469], Reconst Loss: 10699.9316, KL Div: 3341.5679\n",
      "Epoch[10/15], Step [370/469], Reconst Loss: 10452.2812, KL Div: 3182.7622\n",
      "Epoch[10/15], Step [380/469], Reconst Loss: 10814.8496, KL Div: 3435.4438\n",
      "Epoch[10/15], Step [390/469], Reconst Loss: 10083.7988, KL Div: 3167.1865\n",
      "Epoch[10/15], Step [400/469], Reconst Loss: 10120.9463, KL Div: 3179.1831\n",
      "Epoch[10/15], Step [410/469], Reconst Loss: 10289.5625, KL Div: 3188.8369\n",
      "Epoch[10/15], Step [420/469], Reconst Loss: 10338.2695, KL Div: 3290.4045\n",
      "Epoch[10/15], Step [430/469], Reconst Loss: 10543.0020, KL Div: 3240.2930\n",
      "Epoch[10/15], Step [440/469], Reconst Loss: 10319.5068, KL Div: 3293.5234\n",
      "Epoch[10/15], Step [450/469], Reconst Loss: 10416.0000, KL Div: 3264.1545\n",
      "Epoch[10/15], Step [460/469], Reconst Loss: 10410.1680, KL Div: 3284.7769\n",
      "Epoch[11/15], Step [10/469], Reconst Loss: 10588.2676, KL Div: 3347.0337\n",
      "Epoch[11/15], Step [20/469], Reconst Loss: 10143.0508, KL Div: 3051.0332\n",
      "Epoch[11/15], Step [30/469], Reconst Loss: 10461.5488, KL Div: 3390.8613\n",
      "Epoch[11/15], Step [40/469], Reconst Loss: 10395.8057, KL Div: 3160.2456\n",
      "Epoch[11/15], Step [50/469], Reconst Loss: 10371.3145, KL Div: 3256.9746\n",
      "Epoch[11/15], Step [60/469], Reconst Loss: 10125.6143, KL Div: 3255.7957\n",
      "Epoch[11/15], Step [70/469], Reconst Loss: 9663.2832, KL Div: 3158.5098\n",
      "Epoch[11/15], Step [80/469], Reconst Loss: 10123.9023, KL Div: 3361.1274\n",
      "Epoch[11/15], Step [90/469], Reconst Loss: 10657.2188, KL Div: 3263.7202\n",
      "Epoch[11/15], Step [100/469], Reconst Loss: 10371.2393, KL Div: 3344.2583\n",
      "Epoch[11/15], Step [110/469], Reconst Loss: 10710.3145, KL Div: 3420.9597\n",
      "Epoch[11/15], Step [120/469], Reconst Loss: 10593.5146, KL Div: 3301.8689\n",
      "Epoch[11/15], Step [130/469], Reconst Loss: 10718.7930, KL Div: 3301.5193\n",
      "Epoch[11/15], Step [140/469], Reconst Loss: 10355.4395, KL Div: 3244.3347\n",
      "Epoch[11/15], Step [150/469], Reconst Loss: 10529.7881, KL Div: 3244.6890\n",
      "Epoch[11/15], Step [160/469], Reconst Loss: 10262.2295, KL Div: 3133.2332\n",
      "Epoch[11/15], Step [170/469], Reconst Loss: 10141.0508, KL Div: 3284.0732\n",
      "Epoch[11/15], Step [180/469], Reconst Loss: 10161.6445, KL Div: 3128.5024\n",
      "Epoch[11/15], Step [190/469], Reconst Loss: 10630.5059, KL Div: 3261.1553\n",
      "Epoch[11/15], Step [200/469], Reconst Loss: 9610.2129, KL Div: 3133.7488\n",
      "Epoch[11/15], Step [210/469], Reconst Loss: 10601.6172, KL Div: 3326.3088\n",
      "Epoch[11/15], Step [220/469], Reconst Loss: 10175.6680, KL Div: 3240.6050\n",
      "Epoch[11/15], Step [230/469], Reconst Loss: 10058.2764, KL Div: 3224.1812\n",
      "Epoch[11/15], Step [240/469], Reconst Loss: 10049.3008, KL Div: 3160.6616\n",
      "Epoch[11/15], Step [250/469], Reconst Loss: 9983.2012, KL Div: 3275.2026\n",
      "Epoch[11/15], Step [260/469], Reconst Loss: 10591.9229, KL Div: 3274.0654\n",
      "Epoch[11/15], Step [270/469], Reconst Loss: 10139.2520, KL Div: 3303.8918\n",
      "Epoch[11/15], Step [280/469], Reconst Loss: 9942.0264, KL Div: 3240.8511\n",
      "Epoch[11/15], Step [290/469], Reconst Loss: 10190.8418, KL Div: 3324.1980\n",
      "Epoch[11/15], Step [300/469], Reconst Loss: 10679.3242, KL Div: 3302.2363\n",
      "Epoch[11/15], Step [310/469], Reconst Loss: 10010.4082, KL Div: 3336.0581\n",
      "Epoch[11/15], Step [320/469], Reconst Loss: 10131.4521, KL Div: 3212.2729\n",
      "Epoch[11/15], Step [330/469], Reconst Loss: 10632.2246, KL Div: 3276.2300\n",
      "Epoch[11/15], Step [340/469], Reconst Loss: 9929.6689, KL Div: 3217.1626\n",
      "Epoch[11/15], Step [350/469], Reconst Loss: 10384.5986, KL Div: 3220.7852\n",
      "Epoch[11/15], Step [360/469], Reconst Loss: 10569.4648, KL Div: 3278.6990\n",
      "Epoch[11/15], Step [370/469], Reconst Loss: 9912.4121, KL Div: 3214.6711\n",
      "Epoch[11/15], Step [380/469], Reconst Loss: 10423.6152, KL Div: 3302.4248\n",
      "Epoch[11/15], Step [390/469], Reconst Loss: 10276.7822, KL Div: 3209.1299\n",
      "Epoch[11/15], Step [400/469], Reconst Loss: 10569.4150, KL Div: 3264.9043\n",
      "Epoch[11/15], Step [410/469], Reconst Loss: 10404.7363, KL Div: 3245.8523\n",
      "Epoch[11/15], Step [420/469], Reconst Loss: 10470.1016, KL Div: 3252.4590\n",
      "Epoch[11/15], Step [430/469], Reconst Loss: 10346.2012, KL Div: 3196.5037\n",
      "Epoch[11/15], Step [440/469], Reconst Loss: 10427.6387, KL Div: 3300.0200\n",
      "Epoch[11/15], Step [450/469], Reconst Loss: 10395.3896, KL Div: 3304.6223\n",
      "Epoch[11/15], Step [460/469], Reconst Loss: 9881.1855, KL Div: 3224.4204\n",
      "Epoch[12/15], Step [10/469], Reconst Loss: 10116.1484, KL Div: 3275.3369\n",
      "Epoch[12/15], Step [20/469], Reconst Loss: 10226.7793, KL Div: 3202.5291\n",
      "Epoch[12/15], Step [30/469], Reconst Loss: 10192.9375, KL Div: 3218.7100\n",
      "Epoch[12/15], Step [40/469], Reconst Loss: 10427.8135, KL Div: 3237.4502\n",
      "Epoch[12/15], Step [50/469], Reconst Loss: 9986.5234, KL Div: 3265.1624\n",
      "Epoch[12/15], Step [60/469], Reconst Loss: 10588.0713, KL Div: 3318.9548\n",
      "Epoch[12/15], Step [70/469], Reconst Loss: 10289.0742, KL Div: 3226.5205\n",
      "Epoch[12/15], Step [80/469], Reconst Loss: 10064.6719, KL Div: 3214.0620\n",
      "Epoch[12/15], Step [90/469], Reconst Loss: 10676.0234, KL Div: 3186.3916\n",
      "Epoch[12/15], Step [100/469], Reconst Loss: 10542.3311, KL Div: 3256.1831\n",
      "Epoch[12/15], Step [110/469], Reconst Loss: 10349.2725, KL Div: 3231.5083\n",
      "Epoch[12/15], Step [120/469], Reconst Loss: 10511.9043, KL Div: 3247.0693\n",
      "Epoch[12/15], Step [130/469], Reconst Loss: 10046.2324, KL Div: 3233.6919\n",
      "Epoch[12/15], Step [140/469], Reconst Loss: 10051.6133, KL Div: 3179.9194\n",
      "Epoch[12/15], Step [150/469], Reconst Loss: 10174.3535, KL Div: 3248.5469\n",
      "Epoch[12/15], Step [160/469], Reconst Loss: 9970.6270, KL Div: 3235.6064\n",
      "Epoch[12/15], Step [170/469], Reconst Loss: 10461.1416, KL Div: 3176.8672\n",
      "Epoch[12/15], Step [180/469], Reconst Loss: 10374.3584, KL Div: 3253.8000\n",
      "Epoch[12/15], Step [190/469], Reconst Loss: 10740.8027, KL Div: 3242.9036\n",
      "Epoch[12/15], Step [200/469], Reconst Loss: 10285.7646, KL Div: 3288.3643\n",
      "Epoch[12/15], Step [210/469], Reconst Loss: 10426.4023, KL Div: 3259.3779\n",
      "Epoch[12/15], Step [220/469], Reconst Loss: 10203.6055, KL Div: 3235.5327\n",
      "Epoch[12/15], Step [230/469], Reconst Loss: 10305.2949, KL Div: 3361.5264\n",
      "Epoch[12/15], Step [240/469], Reconst Loss: 9878.6943, KL Div: 3166.4939\n",
      "Epoch[12/15], Step [250/469], Reconst Loss: 10202.2031, KL Div: 3147.3213\n",
      "Epoch[12/15], Step [260/469], Reconst Loss: 10066.0049, KL Div: 3248.8423\n",
      "Epoch[12/15], Step [270/469], Reconst Loss: 10618.3760, KL Div: 3214.9336\n",
      "Epoch[12/15], Step [280/469], Reconst Loss: 9865.0244, KL Div: 3248.6099\n",
      "Epoch[12/15], Step [290/469], Reconst Loss: 10529.6309, KL Div: 3222.5447\n",
      "Epoch[12/15], Step [300/469], Reconst Loss: 9891.9424, KL Div: 3241.4487\n",
      "Epoch[12/15], Step [310/469], Reconst Loss: 10550.0605, KL Div: 3273.0154\n",
      "Epoch[12/15], Step [320/469], Reconst Loss: 10107.0020, KL Div: 3224.6738\n",
      "Epoch[12/15], Step [330/469], Reconst Loss: 10264.3945, KL Div: 3266.9221\n",
      "Epoch[12/15], Step [340/469], Reconst Loss: 10512.5605, KL Div: 3205.7632\n",
      "Epoch[12/15], Step [350/469], Reconst Loss: 10210.5684, KL Div: 3314.0789\n",
      "Epoch[12/15], Step [360/469], Reconst Loss: 9946.8594, KL Div: 3220.8308\n",
      "Epoch[12/15], Step [370/469], Reconst Loss: 9963.3721, KL Div: 3180.2498\n",
      "Epoch[12/15], Step [380/469], Reconst Loss: 10315.1982, KL Div: 3316.1731\n",
      "Epoch[12/15], Step [390/469], Reconst Loss: 10441.1621, KL Div: 3204.3987\n",
      "Epoch[12/15], Step [400/469], Reconst Loss: 10652.4238, KL Div: 3341.1907\n",
      "Epoch[12/15], Step [410/469], Reconst Loss: 10418.4785, KL Div: 3283.1814\n",
      "Epoch[12/15], Step [420/469], Reconst Loss: 10460.3408, KL Div: 3366.2119\n",
      "Epoch[12/15], Step [430/469], Reconst Loss: 10923.8740, KL Div: 3317.7029\n",
      "Epoch[12/15], Step [440/469], Reconst Loss: 10310.0742, KL Div: 3348.1768\n",
      "Epoch[12/15], Step [450/469], Reconst Loss: 10188.4199, KL Div: 3263.7939\n",
      "Epoch[12/15], Step [460/469], Reconst Loss: 10621.7783, KL Div: 3333.7896\n",
      "Epoch[13/15], Step [10/469], Reconst Loss: 10344.2676, KL Div: 3290.2339\n",
      "Epoch[13/15], Step [20/469], Reconst Loss: 10360.2646, KL Div: 3317.5879\n",
      "Epoch[13/15], Step [30/469], Reconst Loss: 10001.0996, KL Div: 3231.0112\n",
      "Epoch[13/15], Step [40/469], Reconst Loss: 10171.0742, KL Div: 3230.6255\n",
      "Epoch[13/15], Step [50/469], Reconst Loss: 10203.0068, KL Div: 3290.3003\n",
      "Epoch[13/15], Step [60/469], Reconst Loss: 10252.1953, KL Div: 3305.6301\n",
      "Epoch[13/15], Step [70/469], Reconst Loss: 10207.7480, KL Div: 3266.9805\n",
      "Epoch[13/15], Step [80/469], Reconst Loss: 10370.6982, KL Div: 3360.9126\n",
      "Epoch[13/15], Step [90/469], Reconst Loss: 9948.7168, KL Div: 3157.6133\n",
      "Epoch[13/15], Step [100/469], Reconst Loss: 9924.8984, KL Div: 3241.7607\n",
      "Epoch[13/15], Step [110/469], Reconst Loss: 10552.7979, KL Div: 3313.6130\n",
      "Epoch[13/15], Step [120/469], Reconst Loss: 10231.0312, KL Div: 3284.0293\n",
      "Epoch[13/15], Step [130/469], Reconst Loss: 10264.3008, KL Div: 3237.1787\n",
      "Epoch[13/15], Step [140/469], Reconst Loss: 10472.2148, KL Div: 3294.7964\n",
      "Epoch[13/15], Step [150/469], Reconst Loss: 10156.0693, KL Div: 3298.7368\n",
      "Epoch[13/15], Step [160/469], Reconst Loss: 10618.5225, KL Div: 3225.6343\n",
      "Epoch[13/15], Step [170/469], Reconst Loss: 10275.1250, KL Div: 3279.2817\n",
      "Epoch[13/15], Step [180/469], Reconst Loss: 10401.1748, KL Div: 3345.9424\n",
      "Epoch[13/15], Step [190/469], Reconst Loss: 9921.3086, KL Div: 3282.5415\n",
      "Epoch[13/15], Step [200/469], Reconst Loss: 9917.4395, KL Div: 3271.8562\n",
      "Epoch[13/15], Step [210/469], Reconst Loss: 10519.2148, KL Div: 3339.3306\n",
      "Epoch[13/15], Step [220/469], Reconst Loss: 10894.5684, KL Div: 3203.3896\n",
      "Epoch[13/15], Step [230/469], Reconst Loss: 10253.7832, KL Div: 3318.3789\n",
      "Epoch[13/15], Step [240/469], Reconst Loss: 10550.8262, KL Div: 3310.1157\n",
      "Epoch[13/15], Step [250/469], Reconst Loss: 10660.2930, KL Div: 3324.3245\n",
      "Epoch[13/15], Step [260/469], Reconst Loss: 10152.0508, KL Div: 3158.3667\n",
      "Epoch[13/15], Step [270/469], Reconst Loss: 9776.6035, KL Div: 3207.0044\n",
      "Epoch[13/15], Step [280/469], Reconst Loss: 10042.1846, KL Div: 3228.0476\n",
      "Epoch[13/15], Step [290/469], Reconst Loss: 10441.7979, KL Div: 3244.3110\n",
      "Epoch[13/15], Step [300/469], Reconst Loss: 10524.6836, KL Div: 3283.2896\n",
      "Epoch[13/15], Step [310/469], Reconst Loss: 10391.1172, KL Div: 3209.8232\n",
      "Epoch[13/15], Step [320/469], Reconst Loss: 10040.9424, KL Div: 3221.2554\n",
      "Epoch[13/15], Step [330/469], Reconst Loss: 10682.0518, KL Div: 3291.2202\n",
      "Epoch[13/15], Step [340/469], Reconst Loss: 10392.5225, KL Div: 3282.2388\n",
      "Epoch[13/15], Step [350/469], Reconst Loss: 10124.2852, KL Div: 3142.2427\n",
      "Epoch[13/15], Step [360/469], Reconst Loss: 10376.3945, KL Div: 3319.4580\n",
      "Epoch[13/15], Step [370/469], Reconst Loss: 9852.6748, KL Div: 3188.6497\n",
      "Epoch[13/15], Step [380/469], Reconst Loss: 10332.1875, KL Div: 3250.9980\n",
      "Epoch[13/15], Step [390/469], Reconst Loss: 10298.2051, KL Div: 3185.3613\n",
      "Epoch[13/15], Step [400/469], Reconst Loss: 9973.2891, KL Div: 3263.2766\n",
      "Epoch[13/15], Step [410/469], Reconst Loss: 10412.6250, KL Div: 3221.5916\n",
      "Epoch[13/15], Step [420/469], Reconst Loss: 10033.8350, KL Div: 3344.4189\n",
      "Epoch[13/15], Step [430/469], Reconst Loss: 10239.1504, KL Div: 3250.9880\n",
      "Epoch[13/15], Step [440/469], Reconst Loss: 10291.1914, KL Div: 3325.4736\n",
      "Epoch[13/15], Step [450/469], Reconst Loss: 10284.1738, KL Div: 3159.1243\n",
      "Epoch[13/15], Step [460/469], Reconst Loss: 10468.0723, KL Div: 3326.1035\n",
      "Epoch[14/15], Step [10/469], Reconst Loss: 10479.8672, KL Div: 3238.3408\n",
      "Epoch[14/15], Step [20/469], Reconst Loss: 10699.8672, KL Div: 3228.7202\n",
      "Epoch[14/15], Step [30/469], Reconst Loss: 10093.9805, KL Div: 3351.6128\n",
      "Epoch[14/15], Step [40/469], Reconst Loss: 10329.0957, KL Div: 3297.9014\n",
      "Epoch[14/15], Step [50/469], Reconst Loss: 9921.7529, KL Div: 3253.3213\n",
      "Epoch[14/15], Step [60/469], Reconst Loss: 10439.3535, KL Div: 3258.7012\n",
      "Epoch[14/15], Step [70/469], Reconst Loss: 10725.9893, KL Div: 3398.8560\n",
      "Epoch[14/15], Step [80/469], Reconst Loss: 10636.8701, KL Div: 3276.8877\n",
      "Epoch[14/15], Step [90/469], Reconst Loss: 10142.8203, KL Div: 3177.8315\n",
      "Epoch[14/15], Step [100/469], Reconst Loss: 10583.8418, KL Div: 3346.5078\n",
      "Epoch[14/15], Step [110/469], Reconst Loss: 9942.3896, KL Div: 3078.5034\n",
      "Epoch[14/15], Step [120/469], Reconst Loss: 10259.4727, KL Div: 3221.5146\n",
      "Epoch[14/15], Step [130/469], Reconst Loss: 9599.9258, KL Div: 3205.2495\n",
      "Epoch[14/15], Step [140/469], Reconst Loss: 10337.3359, KL Div: 3252.9385\n",
      "Epoch[14/15], Step [150/469], Reconst Loss: 10484.8359, KL Div: 3298.3569\n",
      "Epoch[14/15], Step [160/469], Reconst Loss: 9988.8066, KL Div: 3134.8286\n",
      "Epoch[14/15], Step [170/469], Reconst Loss: 10233.6875, KL Div: 3361.8740\n",
      "Epoch[14/15], Step [180/469], Reconst Loss: 10315.2500, KL Div: 3219.2539\n",
      "Epoch[14/15], Step [190/469], Reconst Loss: 9734.5840, KL Div: 3122.2891\n",
      "Epoch[14/15], Step [200/469], Reconst Loss: 10246.5215, KL Div: 3239.6392\n",
      "Epoch[14/15], Step [210/469], Reconst Loss: 10514.3320, KL Div: 3272.9382\n",
      "Epoch[14/15], Step [220/469], Reconst Loss: 9819.2480, KL Div: 3131.6372\n",
      "Epoch[14/15], Step [230/469], Reconst Loss: 9895.2959, KL Div: 3266.5293\n",
      "Epoch[14/15], Step [240/469], Reconst Loss: 9985.0859, KL Div: 3293.5591\n",
      "Epoch[14/15], Step [250/469], Reconst Loss: 10073.0225, KL Div: 3197.5452\n",
      "Epoch[14/15], Step [260/469], Reconst Loss: 10086.6445, KL Div: 3375.2192\n",
      "Epoch[14/15], Step [270/469], Reconst Loss: 9975.0771, KL Div: 3151.8228\n",
      "Epoch[14/15], Step [280/469], Reconst Loss: 10428.3760, KL Div: 3265.6816\n",
      "Epoch[14/15], Step [290/469], Reconst Loss: 10618.4922, KL Div: 3263.2776\n",
      "Epoch[14/15], Step [300/469], Reconst Loss: 10961.8945, KL Div: 3371.5728\n",
      "Epoch[14/15], Step [310/469], Reconst Loss: 9944.8789, KL Div: 3203.4631\n",
      "Epoch[14/15], Step [320/469], Reconst Loss: 10465.3467, KL Div: 3389.1265\n",
      "Epoch[14/15], Step [330/469], Reconst Loss: 9837.6270, KL Div: 3270.0959\n",
      "Epoch[14/15], Step [340/469], Reconst Loss: 9783.4727, KL Div: 3091.2319\n",
      "Epoch[14/15], Step [350/469], Reconst Loss: 10102.0010, KL Div: 3248.0957\n",
      "Epoch[14/15], Step [360/469], Reconst Loss: 10156.4727, KL Div: 3265.7634\n",
      "Epoch[14/15], Step [370/469], Reconst Loss: 10581.9727, KL Div: 3351.6218\n",
      "Epoch[14/15], Step [380/469], Reconst Loss: 10154.3389, KL Div: 3264.4368\n",
      "Epoch[14/15], Step [390/469], Reconst Loss: 10320.8994, KL Div: 3353.9551\n",
      "Epoch[14/15], Step [400/469], Reconst Loss: 10425.5879, KL Div: 3151.0962\n",
      "Epoch[14/15], Step [410/469], Reconst Loss: 10556.4648, KL Div: 3293.9873\n",
      "Epoch[14/15], Step [420/469], Reconst Loss: 10032.7148, KL Div: 3283.1926\n",
      "Epoch[14/15], Step [430/469], Reconst Loss: 10228.9746, KL Div: 3158.9946\n",
      "Epoch[14/15], Step [440/469], Reconst Loss: 10846.3027, KL Div: 3385.7483\n",
      "Epoch[14/15], Step [450/469], Reconst Loss: 9661.9688, KL Div: 3265.5127\n",
      "Epoch[14/15], Step [460/469], Reconst Loss: 10136.9844, KL Div: 3141.2700\n",
      "Epoch[15/15], Step [10/469], Reconst Loss: 10191.1680, KL Div: 3274.8599\n",
      "Epoch[15/15], Step [20/469], Reconst Loss: 11013.6045, KL Div: 3361.1545\n",
      "Epoch[15/15], Step [30/469], Reconst Loss: 9942.5186, KL Div: 3225.0234\n",
      "Epoch[15/15], Step [40/469], Reconst Loss: 9943.8828, KL Div: 3314.7236\n",
      "Epoch[15/15], Step [50/469], Reconst Loss: 10278.8145, KL Div: 3301.1953\n",
      "Epoch[15/15], Step [60/469], Reconst Loss: 10188.2012, KL Div: 3274.8286\n",
      "Epoch[15/15], Step [70/469], Reconst Loss: 9878.1348, KL Div: 3275.5410\n",
      "Epoch[15/15], Step [80/469], Reconst Loss: 9742.4219, KL Div: 3179.1187\n",
      "Epoch[15/15], Step [90/469], Reconst Loss: 10717.2129, KL Div: 3261.6294\n",
      "Epoch[15/15], Step [100/469], Reconst Loss: 9956.6904, KL Div: 3215.0686\n",
      "Epoch[15/15], Step [110/469], Reconst Loss: 10161.5947, KL Div: 3182.6016\n",
      "Epoch[15/15], Step [120/469], Reconst Loss: 10358.3838, KL Div: 3233.9761\n",
      "Epoch[15/15], Step [130/469], Reconst Loss: 10401.7617, KL Div: 3312.2134\n",
      "Epoch[15/15], Step [140/469], Reconst Loss: 10231.7061, KL Div: 3278.9971\n",
      "Epoch[15/15], Step [150/469], Reconst Loss: 10200.3320, KL Div: 3071.9243\n",
      "Epoch[15/15], Step [160/469], Reconst Loss: 9894.7227, KL Div: 3191.3159\n",
      "Epoch[15/15], Step [170/469], Reconst Loss: 10563.3916, KL Div: 3330.9922\n",
      "Epoch[15/15], Step [180/469], Reconst Loss: 10195.0352, KL Div: 3235.1816\n",
      "Epoch[15/15], Step [190/469], Reconst Loss: 10028.4375, KL Div: 3243.0361\n",
      "Epoch[15/15], Step [200/469], Reconst Loss: 10301.1328, KL Div: 3248.6499\n",
      "Epoch[15/15], Step [210/469], Reconst Loss: 10453.4268, KL Div: 3323.1089\n",
      "Epoch[15/15], Step [220/469], Reconst Loss: 10154.3105, KL Div: 3337.8989\n",
      "Epoch[15/15], Step [230/469], Reconst Loss: 9646.4277, KL Div: 3180.8828\n",
      "Epoch[15/15], Step [240/469], Reconst Loss: 9861.6533, KL Div: 3205.8418\n",
      "Epoch[15/15], Step [250/469], Reconst Loss: 10343.8086, KL Div: 3151.0583\n",
      "Epoch[15/15], Step [260/469], Reconst Loss: 10485.8027, KL Div: 3350.2170\n",
      "Epoch[15/15], Step [270/469], Reconst Loss: 10371.8750, KL Div: 3307.5476\n",
      "Epoch[15/15], Step [280/469], Reconst Loss: 9722.5664, KL Div: 3205.5759\n",
      "Epoch[15/15], Step [290/469], Reconst Loss: 9688.7461, KL Div: 3124.6733\n",
      "Epoch[15/15], Step [300/469], Reconst Loss: 9847.2041, KL Div: 3158.2698\n",
      "Epoch[15/15], Step [310/469], Reconst Loss: 9791.0869, KL Div: 3173.7454\n",
      "Epoch[15/15], Step [320/469], Reconst Loss: 9959.3984, KL Div: 3229.0073\n",
      "Epoch[15/15], Step [330/469], Reconst Loss: 10080.9883, KL Div: 3280.7891\n",
      "Epoch[15/15], Step [340/469], Reconst Loss: 10179.1426, KL Div: 3286.4077\n",
      "Epoch[15/15], Step [350/469], Reconst Loss: 9887.5918, KL Div: 3134.9175\n",
      "Epoch[15/15], Step [360/469], Reconst Loss: 9931.8096, KL Div: 3256.0076\n",
      "Epoch[15/15], Step [370/469], Reconst Loss: 10149.5488, KL Div: 3286.2820\n",
      "Epoch[15/15], Step [380/469], Reconst Loss: 9924.3770, KL Div: 3214.3203\n",
      "Epoch[15/15], Step [390/469], Reconst Loss: 10035.7031, KL Div: 3140.0371\n",
      "Epoch[15/15], Step [400/469], Reconst Loss: 10288.1777, KL Div: 3249.5020\n",
      "Epoch[15/15], Step [410/469], Reconst Loss: 10216.3682, KL Div: 3259.9761\n",
      "Epoch[15/15], Step [420/469], Reconst Loss: 10395.3105, KL Div: 3260.8354\n",
      "Epoch[15/15], Step [430/469], Reconst Loss: 10017.6094, KL Div: 3261.1699\n",
      "Epoch[15/15], Step [440/469], Reconst Loss: 10666.1533, KL Div: 3306.0308\n",
      "Epoch[15/15], Step [450/469], Reconst Loss: 9963.7637, KL Div: 3197.0344\n",
      "Epoch[15/15], Step [460/469], Reconst Loss: 9974.6416, KL Div: 3327.9780\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x, _) in enumerate(data_loader):\n",
    "        # Forward pass\n",
    "        x = x.to(device).view(-1, image_size)\n",
    "        x_reconst, mu, log_var = model(x)\n",
    "        \n",
    "        # Compute reconstruction loss and kl divergence\n",
    "        # For KL divergence, see Appendix B in VAE paper or http://yunjey47.tistory.com/43\n",
    "        reconst_loss = F.binary_cross_entropy(x_reconst, x, size_average=False)\n",
    "        kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        loss = reconst_loss + kl_div\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}\" \n",
    "                   .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item(), kl_div.item()))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Save the sampled images\n",
    "        z = torch.randn(batch_size, z_dim).to(device)\n",
    "        out = model.decode(z).view(-1, 1, 28, 28)\n",
    "        save_image(out, os.path.join(sample_dir, 'sampled-{}.png'.format(epoch+1)))\n",
    "\n",
    "        # Save the reconstructed images\n",
    "        out, _, _ = model(x)\n",
    "        x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=3)\n",
    "        save_image(x_concat, os.path.join(sample_dir, 'reconst-{}.png'.format(epoch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}