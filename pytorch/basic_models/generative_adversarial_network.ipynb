{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0cc86fbc300e6d473e5cf532321586f93c8d337e756c8fe34a22a2b65e82a1ba0",
   "display_name": "Python 3.7.9 64-bit ('piprnn': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "生成对抗网络: MNIST图像生成\n",
    "    只使用训练数据，训练时创建伪标签\n",
    "    分别定义生成器和判别器，三个线性层\n",
    "    使用二元交叉熵损失\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "latent_size = 64\n",
    "hidden_size = 256\n",
    "image_size = 784\n",
    "num_epochs = 200\n",
    "batch_size = 100\n",
    "sample_dir = 'gan_samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory if not exists\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image processing\n",
    "# transform = transforms.Compose([\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize(mean=(0.5, 0.5, 0.5),   # 3 for RGB channels\n",
    "#                                      std=(0.5, 0.5, 0.5))])\n",
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5],   # 1 for greyscale channels\n",
    "                                     std=[0.5])])\n",
    "\n",
    "# MNIST dataset\n",
    "mnist = torchvision.datasets.MNIST(root='../data/',\n",
    "                                   train=True,\n",
    "                                   transform=transform,\n",
    "                                   download=True)\n",
    "\n",
    "# Data loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "D = nn.Sequential(\n",
    "    nn.Linear(image_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, 1),\n",
    "    nn.Sigmoid())\n",
    "\n",
    "# Generator \n",
    "G = nn.Sequential(\n",
    "    nn.Linear(latent_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, image_size),\n",
    "    nn.Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setting\n",
    "D = D.to(device)\n",
    "G = G.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary cross entropy loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)\n",
    "\n",
    "def reset_grad():\n",
    "    d_optimizer.zero_grad()\n",
    "    g_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "p [600/600], d_loss: 0.8820, g_loss: 1.6612, D(x): 0.71, D(G(z)): 0.28\n",
      "Epoch [127/200], Step [200/600], d_loss: 0.8783, g_loss: 1.4386, D(x): 0.80, D(G(z)): 0.39\n",
      "Epoch [127/200], Step [400/600], d_loss: 0.8301, g_loss: 2.1521, D(x): 0.72, D(G(z)): 0.27\n",
      "Epoch [127/200], Step [600/600], d_loss: 0.8308, g_loss: 1.6927, D(x): 0.68, D(G(z)): 0.25\n",
      "Epoch [128/200], Step [200/600], d_loss: 0.9707, g_loss: 1.6468, D(x): 0.68, D(G(z)): 0.33\n",
      "Epoch [128/200], Step [400/600], d_loss: 0.8340, g_loss: 2.0012, D(x): 0.68, D(G(z)): 0.21\n",
      "Epoch [128/200], Step [600/600], d_loss: 0.9995, g_loss: 1.2365, D(x): 0.74, D(G(z)): 0.40\n",
      "Epoch [129/200], Step [200/600], d_loss: 0.8794, g_loss: 1.8381, D(x): 0.67, D(G(z)): 0.26\n",
      "Epoch [129/200], Step [400/600], d_loss: 0.8546, g_loss: 1.4201, D(x): 0.68, D(G(z)): 0.28\n",
      "Epoch [129/200], Step [600/600], d_loss: 0.8832, g_loss: 1.7487, D(x): 0.70, D(G(z)): 0.31\n",
      "Epoch [130/200], Step [200/600], d_loss: 0.7893, g_loss: 1.9768, D(x): 0.71, D(G(z)): 0.26\n",
      "Epoch [130/200], Step [400/600], d_loss: 0.9527, g_loss: 1.6253, D(x): 0.74, D(G(z)): 0.37\n",
      "Epoch [130/200], Step [600/600], d_loss: 1.2800, g_loss: 1.8178, D(x): 0.61, D(G(z)): 0.37\n",
      "Epoch [131/200], Step [200/600], d_loss: 0.9604, g_loss: 1.4713, D(x): 0.72, D(G(z)): 0.36\n",
      "Epoch [131/200], Step [400/600], d_loss: 0.9505, g_loss: 2.1042, D(x): 0.66, D(G(z)): 0.25\n",
      "Epoch [131/200], Step [600/600], d_loss: 0.8884, g_loss: 2.0901, D(x): 0.68, D(G(z)): 0.26\n",
      "Epoch [132/200], Step [200/600], d_loss: 0.9029, g_loss: 1.9761, D(x): 0.67, D(G(z)): 0.26\n",
      "Epoch [132/200], Step [400/600], d_loss: 0.9579, g_loss: 1.5457, D(x): 0.64, D(G(z)): 0.27\n",
      "Epoch [132/200], Step [600/600], d_loss: 1.0101, g_loss: 1.4039, D(x): 0.70, D(G(z)): 0.37\n",
      "Epoch [133/200], Step [200/600], d_loss: 1.0005, g_loss: 1.9711, D(x): 0.68, D(G(z)): 0.34\n",
      "Epoch [133/200], Step [400/600], d_loss: 0.9551, g_loss: 1.5004, D(x): 0.76, D(G(z)): 0.36\n",
      "Epoch [133/200], Step [600/600], d_loss: 0.7837, g_loss: 1.5219, D(x): 0.74, D(G(z)): 0.28\n",
      "Epoch [134/200], Step [200/600], d_loss: 1.0499, g_loss: 1.4229, D(x): 0.67, D(G(z)): 0.35\n",
      "Epoch [134/200], Step [400/600], d_loss: 1.0483, g_loss: 1.3672, D(x): 0.68, D(G(z)): 0.36\n",
      "Epoch [134/200], Step [600/600], d_loss: 0.6955, g_loss: 1.5699, D(x): 0.78, D(G(z)): 0.28\n",
      "Epoch [135/200], Step [200/600], d_loss: 0.9785, g_loss: 1.5691, D(x): 0.68, D(G(z)): 0.33\n",
      "Epoch [135/200], Step [400/600], d_loss: 0.9388, g_loss: 1.4412, D(x): 0.69, D(G(z)): 0.28\n",
      "Epoch [135/200], Step [600/600], d_loss: 0.8554, g_loss: 1.3324, D(x): 0.77, D(G(z)): 0.36\n",
      "Epoch [136/200], Step [200/600], d_loss: 0.9732, g_loss: 1.4811, D(x): 0.70, D(G(z)): 0.35\n",
      "Epoch [136/200], Step [400/600], d_loss: 0.8249, g_loss: 1.3940, D(x): 0.71, D(G(z)): 0.30\n",
      "Epoch [136/200], Step [600/600], d_loss: 0.7592, g_loss: 1.9762, D(x): 0.72, D(G(z)): 0.24\n",
      "Epoch [137/200], Step [200/600], d_loss: 0.7306, g_loss: 1.5815, D(x): 0.81, D(G(z)): 0.34\n",
      "Epoch [137/200], Step [400/600], d_loss: 0.7616, g_loss: 1.2710, D(x): 0.77, D(G(z)): 0.32\n",
      "Epoch [137/200], Step [600/600], d_loss: 0.8200, g_loss: 1.5225, D(x): 0.79, D(G(z)): 0.30\n",
      "Epoch [138/200], Step [200/600], d_loss: 0.8418, g_loss: 1.3998, D(x): 0.70, D(G(z)): 0.28\n",
      "Epoch [138/200], Step [400/600], d_loss: 1.0759, g_loss: 1.9840, D(x): 0.72, D(G(z)): 0.38\n",
      "Epoch [138/200], Step [600/600], d_loss: 1.0689, g_loss: 1.8096, D(x): 0.70, D(G(z)): 0.37\n",
      "Epoch [139/200], Step [200/600], d_loss: 0.8472, g_loss: 1.6984, D(x): 0.68, D(G(z)): 0.28\n",
      "Epoch [139/200], Step [400/600], d_loss: 0.9477, g_loss: 1.8237, D(x): 0.68, D(G(z)): 0.30\n",
      "Epoch [139/200], Step [600/600], d_loss: 0.9916, g_loss: 1.5964, D(x): 0.68, D(G(z)): 0.30\n",
      "Epoch [140/200], Step [200/600], d_loss: 1.1763, g_loss: 1.6005, D(x): 0.67, D(G(z)): 0.37\n",
      "Epoch [140/200], Step [400/600], d_loss: 0.9498, g_loss: 1.4451, D(x): 0.75, D(G(z)): 0.38\n",
      "Epoch [140/200], Step [600/600], d_loss: 1.1381, g_loss: 1.3974, D(x): 0.70, D(G(z)): 0.42\n",
      "Epoch [141/200], Step [200/600], d_loss: 1.0738, g_loss: 1.2682, D(x): 0.67, D(G(z)): 0.37\n",
      "Epoch [141/200], Step [400/600], d_loss: 0.8431, g_loss: 1.3008, D(x): 0.69, D(G(z)): 0.27\n",
      "Epoch [141/200], Step [600/600], d_loss: 0.8285, g_loss: 1.4680, D(x): 0.72, D(G(z)): 0.27\n",
      "Epoch [142/200], Step [200/600], d_loss: 1.0992, g_loss: 1.6639, D(x): 0.64, D(G(z)): 0.34\n",
      "Epoch [142/200], Step [400/600], d_loss: 0.9498, g_loss: 1.3048, D(x): 0.75, D(G(z)): 0.39\n",
      "Epoch [142/200], Step [600/600], d_loss: 0.9894, g_loss: 1.2726, D(x): 0.61, D(G(z)): 0.25\n",
      "Epoch [143/200], Step [200/600], d_loss: 0.9497, g_loss: 1.5647, D(x): 0.74, D(G(z)): 0.37\n",
      "Epoch [143/200], Step [400/600], d_loss: 0.9326, g_loss: 1.3191, D(x): 0.80, D(G(z)): 0.42\n",
      "Epoch [143/200], Step [600/600], d_loss: 1.1107, g_loss: 1.6598, D(x): 0.67, D(G(z)): 0.33\n",
      "Epoch [144/200], Step [200/600], d_loss: 0.8098, g_loss: 1.7072, D(x): 0.78, D(G(z)): 0.34\n",
      "Epoch [144/200], Step [400/600], d_loss: 0.7893, g_loss: 1.8542, D(x): 0.75, D(G(z)): 0.30\n",
      "Epoch [144/200], Step [600/600], d_loss: 0.7786, g_loss: 1.5990, D(x): 0.73, D(G(z)): 0.28\n",
      "Epoch [145/200], Step [200/600], d_loss: 0.8541, g_loss: 1.2439, D(x): 0.77, D(G(z)): 0.38\n",
      "Epoch [145/200], Step [400/600], d_loss: 0.9031, g_loss: 1.8211, D(x): 0.69, D(G(z)): 0.27\n",
      "Epoch [145/200], Step [600/600], d_loss: 0.9570, g_loss: 1.6680, D(x): 0.65, D(G(z)): 0.26\n",
      "Epoch [146/200], Step [200/600], d_loss: 1.0374, g_loss: 1.4205, D(x): 0.64, D(G(z)): 0.29\n",
      "Epoch [146/200], Step [400/600], d_loss: 0.8867, g_loss: 1.3606, D(x): 0.71, D(G(z)): 0.31\n",
      "Epoch [146/200], Step [600/600], d_loss: 0.8882, g_loss: 1.2628, D(x): 0.68, D(G(z)): 0.29\n",
      "Epoch [147/200], Step [200/600], d_loss: 0.8766, g_loss: 1.5888, D(x): 0.72, D(G(z)): 0.34\n",
      "Epoch [147/200], Step [400/600], d_loss: 1.0235, g_loss: 1.7545, D(x): 0.62, D(G(z)): 0.29\n",
      "Epoch [147/200], Step [600/600], d_loss: 1.2122, g_loss: 1.3435, D(x): 0.68, D(G(z)): 0.40\n",
      "Epoch [148/200], Step [200/600], d_loss: 0.8538, g_loss: 1.3657, D(x): 0.78, D(G(z)): 0.35\n",
      "Epoch [148/200], Step [400/600], d_loss: 0.8277, g_loss: 1.3043, D(x): 0.72, D(G(z)): 0.31\n",
      "Epoch [148/200], Step [600/600], d_loss: 0.8582, g_loss: 1.3576, D(x): 0.74, D(G(z)): 0.31\n",
      "Epoch [149/200], Step [200/600], d_loss: 0.7588, g_loss: 1.8746, D(x): 0.76, D(G(z)): 0.29\n",
      "Epoch [149/200], Step [400/600], d_loss: 0.9927, g_loss: 1.6501, D(x): 0.71, D(G(z)): 0.31\n",
      "Epoch [149/200], Step [600/600], d_loss: 0.9633, g_loss: 1.7865, D(x): 0.60, D(G(z)): 0.24\n",
      "Epoch [150/200], Step [200/600], d_loss: 1.1129, g_loss: 1.6886, D(x): 0.60, D(G(z)): 0.26\n",
      "Epoch [150/200], Step [400/600], d_loss: 0.9742, g_loss: 1.6200, D(x): 0.73, D(G(z)): 0.35\n",
      "Epoch [150/200], Step [600/600], d_loss: 1.1684, g_loss: 1.6381, D(x): 0.65, D(G(z)): 0.37\n",
      "Epoch [151/200], Step [200/600], d_loss: 1.0240, g_loss: 1.9066, D(x): 0.61, D(G(z)): 0.26\n",
      "Epoch [151/200], Step [400/600], d_loss: 0.8859, g_loss: 1.9587, D(x): 0.77, D(G(z)): 0.38\n",
      "Epoch [151/200], Step [600/600], d_loss: 0.9838, g_loss: 1.8665, D(x): 0.63, D(G(z)): 0.25\n",
      "Epoch [152/200], Step [200/600], d_loss: 0.8885, g_loss: 1.6864, D(x): 0.74, D(G(z)): 0.31\n",
      "Epoch [152/200], Step [400/600], d_loss: 1.1758, g_loss: 1.7615, D(x): 0.73, D(G(z)): 0.39\n",
      "Epoch [152/200], Step [600/600], d_loss: 1.0102, g_loss: 1.6256, D(x): 0.70, D(G(z)): 0.35\n",
      "Epoch [153/200], Step [200/600], d_loss: 1.2141, g_loss: 1.5539, D(x): 0.60, D(G(z)): 0.35\n",
      "Epoch [153/200], Step [400/600], d_loss: 0.9169, g_loss: 1.3979, D(x): 0.70, D(G(z)): 0.32\n",
      "Epoch [153/200], Step [600/600], d_loss: 0.9046, g_loss: 1.6648, D(x): 0.70, D(G(z)): 0.32\n",
      "Epoch [154/200], Step [200/600], d_loss: 1.1580, g_loss: 1.5260, D(x): 0.62, D(G(z)): 0.34\n",
      "Epoch [154/200], Step [400/600], d_loss: 0.8809, g_loss: 1.3756, D(x): 0.73, D(G(z)): 0.34\n",
      "Epoch [154/200], Step [600/600], d_loss: 1.1211, g_loss: 1.2719, D(x): 0.65, D(G(z)): 0.35\n",
      "Epoch [155/200], Step [200/600], d_loss: 0.9636, g_loss: 1.6960, D(x): 0.64, D(G(z)): 0.23\n",
      "Epoch [155/200], Step [400/600], d_loss: 0.8403, g_loss: 1.6010, D(x): 0.71, D(G(z)): 0.32\n",
      "Epoch [155/200], Step [600/600], d_loss: 1.1120, g_loss: 1.3546, D(x): 0.64, D(G(z)): 0.32\n",
      "Epoch [156/200], Step [200/600], d_loss: 0.8782, g_loss: 1.5068, D(x): 0.74, D(G(z)): 0.32\n",
      "Epoch [156/200], Step [400/600], d_loss: 0.8899, g_loss: 1.3705, D(x): 0.73, D(G(z)): 0.34\n",
      "Epoch [156/200], Step [600/600], d_loss: 0.8404, g_loss: 1.6934, D(x): 0.72, D(G(z)): 0.28\n",
      "Epoch [157/200], Step [200/600], d_loss: 0.9493, g_loss: 1.7355, D(x): 0.69, D(G(z)): 0.29\n",
      "Epoch [157/200], Step [400/600], d_loss: 0.9073, g_loss: 1.2170, D(x): 0.76, D(G(z)): 0.36\n",
      "Epoch [157/200], Step [600/600], d_loss: 1.1182, g_loss: 1.3229, D(x): 0.62, D(G(z)): 0.32\n",
      "Epoch [158/200], Step [200/600], d_loss: 1.0769, g_loss: 1.5290, D(x): 0.68, D(G(z)): 0.36\n",
      "Epoch [158/200], Step [400/600], d_loss: 0.8256, g_loss: 1.6890, D(x): 0.66, D(G(z)): 0.21\n",
      "Epoch [158/200], Step [600/600], d_loss: 0.9982, g_loss: 1.7984, D(x): 0.61, D(G(z)): 0.23\n",
      "Epoch [159/200], Step [200/600], d_loss: 0.7586, g_loss: 1.5972, D(x): 0.72, D(G(z)): 0.25\n",
      "Epoch [159/200], Step [400/600], d_loss: 1.0067, g_loss: 1.4269, D(x): 0.70, D(G(z)): 0.36\n",
      "Epoch [159/200], Step [600/600], d_loss: 1.0019, g_loss: 1.8904, D(x): 0.60, D(G(z)): 0.25\n",
      "Epoch [160/200], Step [200/600], d_loss: 0.8898, g_loss: 1.8085, D(x): 0.72, D(G(z)): 0.33\n",
      "Epoch [160/200], Step [400/600], d_loss: 1.0879, g_loss: 1.4529, D(x): 0.66, D(G(z)): 0.35\n",
      "Epoch [160/200], Step [600/600], d_loss: 1.0976, g_loss: 1.4395, D(x): 0.56, D(G(z)): 0.24\n",
      "Epoch [161/200], Step [200/600], d_loss: 0.9840, g_loss: 1.7190, D(x): 0.68, D(G(z)): 0.33\n",
      "Epoch [161/200], Step [400/600], d_loss: 0.9122, g_loss: 1.4717, D(x): 0.69, D(G(z)): 0.30\n",
      "Epoch [161/200], Step [600/600], d_loss: 0.7828, g_loss: 1.9396, D(x): 0.71, D(G(z)): 0.26\n",
      "Epoch [162/200], Step [200/600], d_loss: 0.8966, g_loss: 1.5515, D(x): 0.71, D(G(z)): 0.32\n",
      "Epoch [162/200], Step [400/600], d_loss: 0.9630, g_loss: 1.4535, D(x): 0.72, D(G(z)): 0.35\n",
      "Epoch [162/200], Step [600/600], d_loss: 0.9765, g_loss: 1.3204, D(x): 0.75, D(G(z)): 0.37\n",
      "Epoch [163/200], Step [200/600], d_loss: 0.8665, g_loss: 1.6284, D(x): 0.66, D(G(z)): 0.26\n",
      "Epoch [163/200], Step [400/600], d_loss: 1.0323, g_loss: 1.1785, D(x): 0.64, D(G(z)): 0.30\n",
      "Epoch [163/200], Step [600/600], d_loss: 1.0788, g_loss: 1.0256, D(x): 0.73, D(G(z)): 0.41\n",
      "Epoch [164/200], Step [200/600], d_loss: 0.9008, g_loss: 1.4709, D(x): 0.74, D(G(z)): 0.34\n",
      "Epoch [164/200], Step [400/600], d_loss: 0.9009, g_loss: 1.1414, D(x): 0.71, D(G(z)): 0.34\n",
      "Epoch [164/200], Step [600/600], d_loss: 0.8721, g_loss: 1.4383, D(x): 0.73, D(G(z)): 0.34\n",
      "Epoch [165/200], Step [200/600], d_loss: 0.9407, g_loss: 1.3533, D(x): 0.64, D(G(z)): 0.27\n",
      "Epoch [165/200], Step [400/600], d_loss: 0.9058, g_loss: 1.6581, D(x): 0.80, D(G(z)): 0.38\n",
      "Epoch [165/200], Step [600/600], d_loss: 1.0136, g_loss: 1.4948, D(x): 0.66, D(G(z)): 0.31\n",
      "Epoch [166/200], Step [200/600], d_loss: 0.9373, g_loss: 1.6808, D(x): 0.68, D(G(z)): 0.30\n",
      "Epoch [166/200], Step [400/600], d_loss: 0.7775, g_loss: 1.8337, D(x): 0.74, D(G(z)): 0.27\n",
      "Epoch [166/200], Step [600/600], d_loss: 0.8953, g_loss: 1.9101, D(x): 0.70, D(G(z)): 0.31\n",
      "Epoch [167/200], Step [200/600], d_loss: 0.9519, g_loss: 1.7870, D(x): 0.61, D(G(z)): 0.22\n",
      "Epoch [167/200], Step [400/600], d_loss: 0.9748, g_loss: 1.5157, D(x): 0.68, D(G(z)): 0.30\n",
      "Epoch [167/200], Step [600/600], d_loss: 1.0906, g_loss: 1.3695, D(x): 0.64, D(G(z)): 0.33\n",
      "Epoch [168/200], Step [200/600], d_loss: 1.0568, g_loss: 1.1498, D(x): 0.69, D(G(z)): 0.38\n",
      "Epoch [168/200], Step [400/600], d_loss: 0.8804, g_loss: 1.7731, D(x): 0.69, D(G(z)): 0.28\n",
      "Epoch [168/200], Step [600/600], d_loss: 0.8523, g_loss: 1.5559, D(x): 0.72, D(G(z)): 0.31\n",
      "Epoch [169/200], Step [200/600], d_loss: 0.8972, g_loss: 1.8828, D(x): 0.67, D(G(z)): 0.30\n",
      "Epoch [169/200], Step [400/600], d_loss: 0.8633, g_loss: 1.4721, D(x): 0.71, D(G(z)): 0.30\n",
      "Epoch [169/200], Step [600/600], d_loss: 0.9517, g_loss: 1.9556, D(x): 0.65, D(G(z)): 0.27\n",
      "Epoch [170/200], Step [200/600], d_loss: 1.0324, g_loss: 1.9434, D(x): 0.72, D(G(z)): 0.38\n",
      "Epoch [170/200], Step [400/600], d_loss: 0.9453, g_loss: 1.4490, D(x): 0.69, D(G(z)): 0.31\n",
      "Epoch [170/200], Step [600/600], d_loss: 0.9455, g_loss: 1.4895, D(x): 0.64, D(G(z)): 0.25\n",
      "Epoch [171/200], Step [200/600], d_loss: 1.0840, g_loss: 1.5731, D(x): 0.65, D(G(z)): 0.32\n",
      "Epoch [171/200], Step [400/600], d_loss: 0.9736, g_loss: 1.1260, D(x): 0.78, D(G(z)): 0.40\n",
      "Epoch [171/200], Step [600/600], d_loss: 0.9899, g_loss: 1.4551, D(x): 0.65, D(G(z)): 0.29\n",
      "Epoch [172/200], Step [200/600], d_loss: 0.8600, g_loss: 1.3412, D(x): 0.70, D(G(z)): 0.29\n",
      "Epoch [172/200], Step [400/600], d_loss: 0.9577, g_loss: 1.4451, D(x): 0.71, D(G(z)): 0.33\n",
      "Epoch [172/200], Step [600/600], d_loss: 0.7676, g_loss: 1.5004, D(x): 0.76, D(G(z)): 0.28\n",
      "Epoch [173/200], Step [200/600], d_loss: 1.0318, g_loss: 1.3383, D(x): 0.61, D(G(z)): 0.28\n",
      "Epoch [173/200], Step [400/600], d_loss: 1.0103, g_loss: 1.6813, D(x): 0.75, D(G(z)): 0.40\n",
      "Epoch [173/200], Step [600/600], d_loss: 0.9973, g_loss: 1.3687, D(x): 0.68, D(G(z)): 0.33\n",
      "Epoch [174/200], Step [200/600], d_loss: 1.0002, g_loss: 1.2938, D(x): 0.68, D(G(z)): 0.33\n",
      "Epoch [174/200], Step [400/600], d_loss: 0.9639, g_loss: 1.5784, D(x): 0.71, D(G(z)): 0.35\n",
      "Epoch [174/200], Step [600/600], d_loss: 1.0058, g_loss: 1.6088, D(x): 0.72, D(G(z)): 0.37\n",
      "Epoch [175/200], Step [200/600], d_loss: 0.7110, g_loss: 1.8922, D(x): 0.78, D(G(z)): 0.28\n",
      "Epoch [175/200], Step [400/600], d_loss: 1.0019, g_loss: 1.4304, D(x): 0.65, D(G(z)): 0.30\n",
      "Epoch [175/200], Step [600/600], d_loss: 0.8853, g_loss: 2.0084, D(x): 0.67, D(G(z)): 0.23\n",
      "Epoch [176/200], Step [200/600], d_loss: 0.8835, g_loss: 1.8280, D(x): 0.67, D(G(z)): 0.28\n",
      "Epoch [176/200], Step [400/600], d_loss: 0.9684, g_loss: 1.2349, D(x): 0.69, D(G(z)): 0.34\n",
      "Epoch [176/200], Step [600/600], d_loss: 1.0353, g_loss: 1.4435, D(x): 0.64, D(G(z)): 0.32\n",
      "Epoch [177/200], Step [200/600], d_loss: 0.9522, g_loss: 1.6560, D(x): 0.71, D(G(z)): 0.34\n",
      "Epoch [177/200], Step [400/600], d_loss: 1.0541, g_loss: 1.3889, D(x): 0.59, D(G(z)): 0.24\n",
      "Epoch [177/200], Step [600/600], d_loss: 0.9038, g_loss: 1.4690, D(x): 0.69, D(G(z)): 0.29\n",
      "Epoch [178/200], Step [200/600], d_loss: 1.1668, g_loss: 1.2603, D(x): 0.65, D(G(z)): 0.37\n",
      "Epoch [178/200], Step [400/600], d_loss: 1.0199, g_loss: 1.3310, D(x): 0.72, D(G(z)): 0.38\n",
      "Epoch [178/200], Step [600/600], d_loss: 1.0218, g_loss: 1.4369, D(x): 0.67, D(G(z)): 0.33\n",
      "Epoch [179/200], Step [200/600], d_loss: 1.0808, g_loss: 1.2965, D(x): 0.62, D(G(z)): 0.32\n",
      "Epoch [179/200], Step [400/600], d_loss: 0.8341, g_loss: 1.5956, D(x): 0.73, D(G(z)): 0.28\n",
      "Epoch [179/200], Step [600/600], d_loss: 0.8097, g_loss: 1.7852, D(x): 0.72, D(G(z)): 0.30\n",
      "Epoch [180/200], Step [200/600], d_loss: 1.0944, g_loss: 1.4880, D(x): 0.60, D(G(z)): 0.29\n",
      "Epoch [180/200], Step [400/600], d_loss: 0.9614, g_loss: 1.8616, D(x): 0.62, D(G(z)): 0.23\n",
      "Epoch [180/200], Step [600/600], d_loss: 1.0840, g_loss: 1.5124, D(x): 0.67, D(G(z)): 0.36\n",
      "Epoch [181/200], Step [200/600], d_loss: 0.9849, g_loss: 1.3641, D(x): 0.70, D(G(z)): 0.37\n",
      "Epoch [181/200], Step [400/600], d_loss: 0.8441, g_loss: 1.6889, D(x): 0.70, D(G(z)): 0.29\n",
      "Epoch [181/200], Step [600/600], d_loss: 0.7940, g_loss: 1.6210, D(x): 0.74, D(G(z)): 0.26\n",
      "Epoch [182/200], Step [200/600], d_loss: 1.0374, g_loss: 1.7009, D(x): 0.68, D(G(z)): 0.35\n",
      "Epoch [182/200], Step [400/600], d_loss: 1.4077, g_loss: 1.3083, D(x): 0.67, D(G(z)): 0.47\n",
      "Epoch [182/200], Step [600/600], d_loss: 1.0578, g_loss: 1.3978, D(x): 0.70, D(G(z)): 0.37\n",
      "Epoch [183/200], Step [200/600], d_loss: 0.8597, g_loss: 1.3287, D(x): 0.70, D(G(z)): 0.26\n",
      "Epoch [183/200], Step [400/600], d_loss: 1.1548, g_loss: 1.3678, D(x): 0.66, D(G(z)): 0.41\n",
      "Epoch [183/200], Step [600/600], d_loss: 0.9518, g_loss: 1.1488, D(x): 0.63, D(G(z)): 0.28\n",
      "Epoch [184/200], Step [200/600], d_loss: 1.0228, g_loss: 1.7261, D(x): 0.65, D(G(z)): 0.31\n",
      "Epoch [184/200], Step [400/600], d_loss: 1.1503, g_loss: 1.4219, D(x): 0.65, D(G(z)): 0.37\n",
      "Epoch [184/200], Step [600/600], d_loss: 0.9012, g_loss: 1.5307, D(x): 0.70, D(G(z)): 0.32\n",
      "Epoch [185/200], Step [200/600], d_loss: 1.0387, g_loss: 1.5254, D(x): 0.74, D(G(z)): 0.41\n",
      "Epoch [185/200], Step [400/600], d_loss: 0.9514, g_loss: 1.5359, D(x): 0.70, D(G(z)): 0.33\n",
      "Epoch [185/200], Step [600/600], d_loss: 1.0374, g_loss: 1.0426, D(x): 0.62, D(G(z)): 0.33\n",
      "Epoch [186/200], Step [200/600], d_loss: 1.1365, g_loss: 1.7561, D(x): 0.68, D(G(z)): 0.37\n",
      "Epoch [186/200], Step [400/600], d_loss: 1.0777, g_loss: 1.4768, D(x): 0.71, D(G(z)): 0.37\n",
      "Epoch [186/200], Step [600/600], d_loss: 0.8829, g_loss: 1.5402, D(x): 0.71, D(G(z)): 0.30\n",
      "Epoch [187/200], Step [200/600], d_loss: 0.9072, g_loss: 1.4112, D(x): 0.63, D(G(z)): 0.25\n",
      "Epoch [187/200], Step [400/600], d_loss: 0.8421, g_loss: 1.8605, D(x): 0.67, D(G(z)): 0.24\n",
      "Epoch [187/200], Step [600/600], d_loss: 0.9652, g_loss: 1.6991, D(x): 0.71, D(G(z)): 0.34\n",
      "Epoch [188/200], Step [200/600], d_loss: 0.9009, g_loss: 2.0090, D(x): 0.71, D(G(z)): 0.34\n",
      "Epoch [188/200], Step [400/600], d_loss: 0.9969, g_loss: 1.4248, D(x): 0.67, D(G(z)): 0.33\n",
      "Epoch [188/200], Step [600/600], d_loss: 0.9785, g_loss: 1.3488, D(x): 0.62, D(G(z)): 0.27\n",
      "Epoch [189/200], Step [200/600], d_loss: 0.8672, g_loss: 1.5011, D(x): 0.67, D(G(z)): 0.26\n",
      "Epoch [189/200], Step [400/600], d_loss: 0.8984, g_loss: 1.5187, D(x): 0.66, D(G(z)): 0.26\n",
      "Epoch [189/200], Step [600/600], d_loss: 0.7690, g_loss: 1.3906, D(x): 0.73, D(G(z)): 0.27\n",
      "Epoch [190/200], Step [200/600], d_loss: 1.1885, g_loss: 1.5252, D(x): 0.62, D(G(z)): 0.36\n",
      "Epoch [190/200], Step [400/600], d_loss: 1.0681, g_loss: 1.3260, D(x): 0.69, D(G(z)): 0.40\n",
      "Epoch [190/200], Step [600/600], d_loss: 1.1355, g_loss: 1.3727, D(x): 0.60, D(G(z)): 0.33\n",
      "Epoch [191/200], Step [200/600], d_loss: 1.0279, g_loss: 1.3761, D(x): 0.69, D(G(z)): 0.34\n",
      "Epoch [191/200], Step [400/600], d_loss: 0.8993, g_loss: 1.6246, D(x): 0.70, D(G(z)): 0.29\n",
      "Epoch [191/200], Step [600/600], d_loss: 0.9727, g_loss: 1.4940, D(x): 0.69, D(G(z)): 0.31\n",
      "Epoch [192/200], Step [200/600], d_loss: 0.8852, g_loss: 1.8835, D(x): 0.73, D(G(z)): 0.33\n",
      "Epoch [192/200], Step [400/600], d_loss: 0.8892, g_loss: 1.7720, D(x): 0.69, D(G(z)): 0.30\n",
      "Epoch [192/200], Step [600/600], d_loss: 0.9137, g_loss: 1.4709, D(x): 0.67, D(G(z)): 0.30\n",
      "Epoch [193/200], Step [200/600], d_loss: 1.1642, g_loss: 1.3383, D(x): 0.63, D(G(z)): 0.38\n",
      "Epoch [193/200], Step [400/600], d_loss: 0.9435, g_loss: 1.5391, D(x): 0.66, D(G(z)): 0.29\n",
      "Epoch [193/200], Step [600/600], d_loss: 1.0408, g_loss: 1.6963, D(x): 0.66, D(G(z)): 0.32\n",
      "Epoch [194/200], Step [200/600], d_loss: 1.0550, g_loss: 1.3410, D(x): 0.64, D(G(z)): 0.32\n",
      "Epoch [194/200], Step [400/600], d_loss: 1.1246, g_loss: 1.7267, D(x): 0.70, D(G(z)): 0.38\n",
      "Epoch [194/200], Step [600/600], d_loss: 1.0362, g_loss: 1.3382, D(x): 0.61, D(G(z)): 0.30\n",
      "Epoch [195/200], Step [200/600], d_loss: 0.8571, g_loss: 1.5062, D(x): 0.69, D(G(z)): 0.29\n",
      "Epoch [195/200], Step [400/600], d_loss: 0.9710, g_loss: 2.0281, D(x): 0.76, D(G(z)): 0.37\n",
      "Epoch [195/200], Step [600/600], d_loss: 0.8435, g_loss: 1.3894, D(x): 0.69, D(G(z)): 0.27\n",
      "Epoch [196/200], Step [200/600], d_loss: 1.0301, g_loss: 1.7159, D(x): 0.68, D(G(z)): 0.32\n",
      "Epoch [196/200], Step [400/600], d_loss: 0.9467, g_loss: 1.8388, D(x): 0.69, D(G(z)): 0.31\n",
      "Epoch [196/200], Step [600/600], d_loss: 1.1356, g_loss: 1.3102, D(x): 0.63, D(G(z)): 0.35\n",
      "Epoch [197/200], Step [200/600], d_loss: 0.9949, g_loss: 1.2798, D(x): 0.67, D(G(z)): 0.34\n",
      "Epoch [197/200], Step [400/600], d_loss: 0.8325, g_loss: 1.6867, D(x): 0.65, D(G(z)): 0.22\n",
      "Epoch [197/200], Step [600/600], d_loss: 0.9531, g_loss: 1.7668, D(x): 0.65, D(G(z)): 0.28\n",
      "Epoch [198/200], Step [200/600], d_loss: 1.0335, g_loss: 1.2625, D(x): 0.71, D(G(z)): 0.40\n",
      "Epoch [198/200], Step [400/600], d_loss: 1.0210, g_loss: 1.2990, D(x): 0.70, D(G(z)): 0.38\n",
      "Epoch [198/200], Step [600/600], d_loss: 1.1054, g_loss: 1.6623, D(x): 0.66, D(G(z)): 0.33\n",
      "Epoch [199/200], Step [200/600], d_loss: 0.6957, g_loss: 1.5249, D(x): 0.76, D(G(z)): 0.28\n",
      "Epoch [199/200], Step [400/600], d_loss: 0.7944, g_loss: 1.7674, D(x): 0.69, D(G(z)): 0.25\n",
      "Epoch [199/200], Step [600/600], d_loss: 0.9882, g_loss: 1.3652, D(x): 0.67, D(G(z)): 0.34\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "total_step = len(data_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, _) in enumerate(data_loader):\n",
    "        images = images.reshape(batch_size, -1).to(device)\n",
    "        \n",
    "        # Create the labels which are later used as input for the BCE loss\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        # ================================================================== #\n",
    "        #                      Train the discriminator                       #\n",
    "        # ================================================================== #\n",
    "\n",
    "        # Compute BCE_Loss using real images where BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))\n",
    "        # Second term of the loss is always zero since real_labels == 1\n",
    "        outputs = D(images)\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "        \n",
    "        # Compute BCELoss using fake images\n",
    "        # First term of the loss is always zero since fake_labels == 0\n",
    "        z = torch.randn(batch_size, latent_size).to(device)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        reset_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # ================================================================== #\n",
    "        #                        Train the generator                         #\n",
    "        # ================================================================== #\n",
    "\n",
    "        # Compute loss with fake images\n",
    "        z = torch.randn(batch_size, latent_size).to(device)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        \n",
    "        # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))\n",
    "        # For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        reset_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        if (i+1) % 200 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n",
    "                  .format(epoch, num_epochs, i+1, total_step, d_loss.item(), g_loss.item(), \n",
    "                          real_score.mean().item(), fake_score.mean().item()))\n",
    "    \n",
    "    # Save real images\n",
    "    if (epoch+1) == 1:\n",
    "        images = images.reshape(images.size(0), 1, 28, 28)\n",
    "        save_image(denorm(images), os.path.join(sample_dir, 'real_images.png'))\n",
    "    \n",
    "    # Save sampled images\n",
    "    fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n",
    "    save_image(denorm(fake_images), os.path.join(sample_dir, 'fake_images-{}.png'.format(epoch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoints \n",
    "torch.save(G.state_dict(), 'G.ckpt')\n",
    "torch.save(D.state_dict(), 'D.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}