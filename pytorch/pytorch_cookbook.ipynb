{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0cc86fbc300e6d473e5cf532321586f93c8d337e756c8fe34a22a2b65e82a1ba0",
   "display_name": "Python 3.7.9 64-bit ('piprnn': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "72506cc1e4b0dce5eb876f653d1c5faf29777724f7cbfb23a9a7a2a764425624"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 基础配置"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 检查 PyTorch 版本"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'1.7.0+cu101'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10.0.130\n7602\nGeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "# cuda版本\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "source": [
    "## 张量处理"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 张量基本信息"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[-7.1653e+26,  4.5752e-41, -7.1653e+26,  4.5752e-41],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n\n        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\ntensor([[[7., 1., 7., 6.],\n         [3., 4., 7., 7.],\n         [5., 0., 8., 3.]],\n\n        [[0., 2., 9., 5.],\n         [8., 2., 5., 1.],\n         [2., 4., 5., 2.]]])\ntensor([[[7., 1., 7.],\n         [6., 3., 4.],\n         [7., 7., 5.],\n         [0., 8., 3.]],\n\n        [[0., 2., 9.],\n         [5., 8., 2.],\n         [5., 1., 2.],\n         [4., 5., 2.]]])\ntensor([[[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]]])\ntensor([[[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# 张量的创建\n",
    "example = torch.Tensor(2, 3, 4)\n",
    "print(example)\n",
    "print(example.random_(10)) # 0-9\n",
    "r = torch.Tensor(example)\n",
    "r.resize_(2, 4, 3)\n",
    "print(r)\n",
    "print(r.zero_())\n",
    "print(example) # 值全变为0，size不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.FloatTensor\ntorch.Size([2, 3, 4])\n3\n24\n"
     ]
    }
   ],
   "source": [
    "# 张量的统计数据\n",
    "print(example.type())\n",
    "print(example.size()) # tuple的一个子类\n",
    "print(example.dim())\n",
    "print(example.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1, 2, 3, 4])\n<class 'torch.Tensor'>\ntorch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([1,2,3,4])\n",
    "print(a)\n",
    "print(type(a))\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0, 0, 0, 0])\ntensor([1, 2, 3, 4])\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])\ntorch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([1,2,3,4])\n",
    "print(a.new_zeros(a.size(0)))\n",
    "print(a)\n",
    "\n",
    "e = torch.eye(3)\n",
    "print(e)\n",
    "print(e.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 1., 1.],\n        [1., 1., 1.]])\ntorch.FloatTensor\ntensor(2.4495)\ntensor(6.)\ntensor([1.4142, 2.8284, 4.2426, 5.6569])\ntensor([5.4772, 5.4772])\ntorch.Size([2, 3, 4])\ntorch.Size([2, 1, 4])\ntorch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# 对输入的tensor求范数\n",
    "a = torch.ones((2,3))  #建立tensor torch.ones(2,3)也可\n",
    "a2 = torch.norm(a)      #默认求2范数\n",
    "a1 = torch.norm(a,p=1)  #指定求1范数\n",
    "print(a)\n",
    "print(a.type())\n",
    "print(a2)\n",
    "print(a1)\n",
    "# 求指定维度上的范数\n",
    "a = torch.tensor([[1, 2, 3, 4],\n",
    "        [1, 2, 3, 4]]).float()  #norm仅支持floatTensor,a是一个2*4的Tensor\n",
    "a0 = torch.norm(a,p=2,dim=0)    #按0维度求2范数\n",
    "a1 = torch.norm(a,p=2,dim=1)    #按1维度求2范数\n",
    "print(a0)\n",
    "print(a1)\n",
    "\n",
    "a = torch.rand(2,3,4) # torch.rand((2,3,4))也可  得到[0, 1)均匀分布。torch.randn得到标准正态分布\n",
    "at = torch.norm(a,p=2,dim=1,keepdim=True)   #保持维度\n",
    "af = torch.norm(a,p=2,dim=1,keepdim=False)  #不保持维度\n",
    "# dim称为缩减的维度，因为norm运算之后，此维度或者消失或者元素个数变为1\n",
    " \n",
    "print(a.shape)\n",
    "print(at.shape)\n",
    "print(af.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.0104, 0.8584],\n        [0.5867, 0.0192]])\ntensor([0.5084, 0.1369, 0.5194, 0.1493])\ntensor([0.5900, 0.8857, 0.6408, 0.1345])\n"
     ]
    }
   ],
   "source": [
    "print(torch.rand(2,2))\n",
    "input = torch.tensor([1,2,3,4]).float() \n",
    "print(torch.rand_like(input)) # 等价于torch.rand(input_size))\n",
    "print(torch.rand(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([2, 2, 3, 3])\ntensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor([1,2,3,4])\n",
    "print(torch.clamp(input, 2, 3)) # 将输入input张量每个元素的夹紧到区间 [min,max]\n",
    "print(input)"
   ]
  },
  {
   "source": [
    "### 数据类型转换"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Set default tensor type. Float in PyTorch is much faster than double.\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "# dtype=torch.long\n",
    "\n",
    "# Type convertions.\n",
    "tensor = tensor.cuda()\n",
    "tensor = tensor.cpu() | .cpu().data.numpy()/.data.cpu().numpy()  .data是获取tensor    把tensor转换成numpy的格式(array)\n",
    "tensor = tensor.float()\n",
    "tensor = tensor.long()\n",
    "'''"
   ]
  },
  {
   "source": [
    "### 将整数标记转换成独热(one-hot)编码"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1],\n        [0, 1, 0]])\ntensor([[5],\n        [3],\n        [7],\n        [1]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([0,1,2,1])\n",
    "N = t.size(0)\n",
    "num_classes = 3\n",
    "one_hot = torch.zeros(N, num_classes).long()\n",
    "one_hot.scatter_(dim=1, index=torch.unsqueeze(t, dim=1), src=torch.ones(N, num_classes).long()) # 将src中数据根据index中的索引按照dim的方向填进input中，index为src中的每一个数据指明在input中的dim方向上的索引，index、src和input的另外一个dim的大小要一致\n",
    "print(one_hot)\n",
    "\n",
    "## scatter_ 放 | gather_ 取\n",
    "input = [\n",
    "    [2, 3, 4, 5, 0, 0],\n",
    "    [1, 4, 3, 0, 0, 0],\n",
    "    [4, 2, 2, 5, 7, 0],\n",
    "    [1, 0, 0, 0, 0, 0]\n",
    "]\n",
    "input = torch.tensor(input)\n",
    "#注意index的类型\n",
    "length = torch.LongTensor([[4],[3],[5],[1]])\n",
    "#index之所以减1,是因为序列维度是从0开始计算的\n",
    "out = torch.gather(input, 1, length-1)\n",
    "print(out)"
   ]
  },
  {
   "source": [
    "### 得到非零/零元素"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1],\n        [2],\n        [3]])\ntensor([[0]])\n3\n1\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([0,1,2,1])\n",
    "print(torch.nonzero(t))             # Index of non-zero elements\n",
    "print(torch.nonzero(t == 0))          # Index of zero elements\n",
    "print(torch.nonzero(t).size(0))       # Number of non-zero elements\n",
    "print(torch.nonzero(t == 0).size(0))  # Number of zero elements"
   ]
  },
  {
   "source": [
    "### torch.Tensor 与 np.ndarray 转换"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# torch.Tensor -> np.ndarray.\n",
    "ndarray = example.cpu().numpy()\n",
    "print(type(ndarray))\n",
    "\n",
    "# np.ndarray -> torch.Tensor.\n",
    "tensor = torch.from_numpy(ndarray).float()\n",
    "# tensor = torch.from_numpy(ndarray.copy()).float()  # If ndarray has negative stride\n",
    "print(type(tensor))"
   ]
  },
  {
   "source": [
    "### 打乱顺序"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tensor = tensor[torch.randperm(tensor.size(0))]  # Shuffle the first dimension\n",
    "'''"
   ]
  },
  {
   "source": [
    "### 水平翻转(逆)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000e+00,  0.0000e+00,  2.0556e+32,  3.0704e-41,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  1.9898e-43,  1.0272e-42,  0.0000e+00,  4.5559e-41]],\n",
       "\n",
       "         [[ 1.6956e-43,  1.1771e-43,  0.0000e+00,  0.0000e+00,  3.5566e+31],\n",
       "          [ 3.0704e-41,  7.0065e-45,  0.0000e+00,  9.8567e+30,  3.0704e-41],\n",
       "          [ 4.0402e+06,  4.5577e-41,  1.5765e-35,  4.5574e-41,  0.0000e+00],\n",
       "          [ 0.0000e+00,  4.2039e-45,  0.0000e+00,  1.4013e-45,  4.5573e-41]],\n",
       "\n",
       "         [[ 1.5718e-35,  4.5574e-41,         nan,         nan,  0.0000e+00],\n",
       "          [ 0.0000e+00,  4.2039e-45,  7.0065e-45,  4.2039e-45,  0.0000e+00],\n",
       "          [ 1.5765e-35,  4.5574e-41,  1.5766e-35,  4.5574e-41,  7.5504e-06],\n",
       "          [ 4.5577e-41,         nan,         nan,  0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 1.8444e+31,  3.0704e-41,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  1.5766e-35,  4.5574e-41,  7.5504e-06,  4.5577e-41],\n",
       "          [        nan,         nan,  0.0000e+00,  0.0000e+00,  1.8500e+31],\n",
       "          [ 3.0704e-41,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  9.6799e+30,  3.0704e-41],\n",
       "          [ 9.6799e+30,  3.0704e-41,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  9.6799e+30,  3.0704e-41,  5.2287e+02,  4.5577e-41]],\n",
       "\n",
       "         [[ 9.8557e+30,  3.0704e-41,  9.6936e+30,  3.0704e-41,  9.6936e+30],\n",
       "          [ 3.0704e-41,  3.3260e+05,  4.5577e-41,  0.0000e+00,  0.0000e+00],\n",
       "          [-2.8157e+37,  4.5573e-41, -2.8279e+37,  4.5573e-41,  1.5584e-35],\n",
       "          [ 4.5574e-41,  1.4767e+06,  4.5577e-41,  1.4864e+06,  4.5577e-41]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "example = torch.Tensor(2, 3, 4, 5)\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000e+00,  3.0704e-41,  2.0556e+32,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 4.5559e-41,  0.0000e+00,  1.0272e-42,  1.9898e-43,  0.0000e+00]],\n",
       "\n",
       "         [[ 3.5566e+31,  0.0000e+00,  0.0000e+00,  1.1771e-43,  1.6956e-43],\n",
       "          [ 3.0704e-41,  9.8567e+30,  0.0000e+00,  7.0065e-45,  3.0704e-41],\n",
       "          [ 0.0000e+00,  4.5574e-41,  1.5765e-35,  4.5577e-41,  4.0402e+06],\n",
       "          [ 4.5573e-41,  1.4013e-45,  0.0000e+00,  4.2039e-45,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,         nan,         nan,  4.5574e-41,  1.5718e-35],\n",
       "          [ 0.0000e+00,  4.2039e-45,  7.0065e-45,  4.2039e-45,  0.0000e+00],\n",
       "          [ 7.5504e-06,  4.5574e-41,  1.5766e-35,  4.5574e-41,  1.5765e-35],\n",
       "          [ 0.0000e+00,  0.0000e+00,         nan,         nan,  4.5577e-41]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  3.0704e-41,  1.8444e+31],\n",
       "          [ 4.5577e-41,  7.5504e-06,  4.5574e-41,  1.5766e-35,  0.0000e+00],\n",
       "          [ 1.8500e+31,  0.0000e+00,  0.0000e+00,         nan,         nan],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.0704e-41]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 3.0704e-41,  9.6799e+30,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  3.0704e-41,  9.6799e+30],\n",
       "          [ 4.5577e-41,  5.2287e+02,  3.0704e-41,  9.6799e+30,  0.0000e+00]],\n",
       "\n",
       "         [[ 9.6936e+30,  3.0704e-41,  9.6936e+30,  3.0704e-41,  9.8557e+30],\n",
       "          [ 0.0000e+00,  0.0000e+00,  4.5577e-41,  3.3260e+05,  3.0704e-41],\n",
       "          [ 1.5584e-35,  4.5573e-41, -2.8279e+37,  4.5573e-41, -2.8157e+37],\n",
       "          [ 4.5577e-41,  1.4864e+06,  4.5577e-41,  1.4767e+06,  4.5574e-41]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "example = example[:, :, :, torch.arange(example.size(3) - 1, -1, -1).long()]\n",
    "example"
   ]
  },
  {
   "source": [
    "### 张量复制"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "有三种复制的方式，对应不同的需求:\n",
    "    1、tensor.clone(): 新内存; 仍在计算图中\n",
    "    2、tensor.detach(): 共享内存; 不在计算图中\n",
    "    3、tensor.detach().clone(): 新内存; 不在计算图中\n",
    "'''"
   ]
  },
  {
   "source": [
    "### 张量转置"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "在使用transpose()进行转置操作时，pytorch并不会创建新的、转置后的tensor，而是修改了tensor中的一些属性(也就是元数据)，使得此时的offset和stride是与转置tensor相对应的。转置的tensor和原tensor的内存是共享的。转置前的是contiguous，转置后的不是   \n",
    "经过转置后得到的tensor，它内部数据的布局方式和从头开始创建一个这样的常规的tensor的布局方式是不一样的    \n",
    "当调用contiguous()时，会强制拷贝一份tensor，让它的布局和从头创建的一模一样\n",
    "\n",
    ".transpose(0, 1) | .permute(1, 0, 2) \n",
    "transpose并不改变a本身的形状，将改变的一个副本赋值给b，相当于先拷贝了一份，然后再改变这份拷贝的\n",
    "permute() 和 tranpose() 比较相似，transpose是交换两个维度，permute()是交换多个维度\n",
    "'''"
   ]
  },
  {
   "source": [
    "### 拼接张量"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([6, 3]) torch.Size([3, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# torch.cat 和 torch.stack 的区别在于 torch.cat 沿着给定的维度拼接，而 torch.stack 会新增一维\n",
    "list_of_tensors = [torch.Tensor(2, 3), torch.Tensor(2, 3), torch.Tensor(2, 3)]\n",
    "t1 = torch.cat(list_of_tensors, dim=0)\n",
    "t2 = torch.stack(list_of_tensors, dim=0)\n",
    "print(t1.shape, t2.shape)"
   ]
  },
  {
   "source": [
    "### 张量扩展"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([2, 4])\ntensor([[0, 1, 2, 1],\n        [2, 4, 1, 4]])\ntensor([[[[0]],\n\n         [[1]]],\n\n\n        [[[2]],\n\n         [[1]]],\n\n\n        [[[2]],\n\n         [[4]]],\n\n\n        [[[1]],\n\n         [[4]]]])\ntorch.Size([4, 2, 2, 2])\ntensor([[[[0, 0],\n          [0, 0]],\n\n         [[1, 1],\n          [1, 1]]],\n\n\n        [[[2, 2],\n          [2, 2]],\n\n         [[1, 1],\n          [1, 1]]],\n\n\n        [[[2, 2],\n          [2, 2]],\n\n         [[4, 4],\n          [4, 4]]],\n\n\n        [[[1, 1],\n          [1, 1]],\n\n         [[4, 4],\n          [4, 4]]]])\ntorch.Size([128, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor有两个实例方法可以用来扩展某维的数据的尺寸，分别是repeat()和expand()\n",
    "# 扩展(expand)张量不会分配新的内存，只是在存在的张量上创建一个新的视图(view)，参数是想要得到的最后张量的形状   \n",
    "# repeat拷贝张量的数据  参数是各个维度上重复的次数\n",
    "t = torch.tensor([[0,1,2,1], [2,4,1,4]])\n",
    "a = torch.reshape(t, (4, 2, 1, 1)).expand(4, 2, 2, 2)\n",
    "print(t.shape)\n",
    "print(t)\n",
    "print(torch.reshape(t, (4, 2, 1, 1)))\n",
    "print(a.shape)\n",
    "print(a)\n",
    "\n",
    "a = torch.Tensor(128,1,512)\n",
    "B = a.repeat(1,5,1)\n",
    "print(B.shape)\n"
   ]
  },
  {
   "source": [
    "### 张量分块"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.7019, 0.3004, 0.6121, 0.8929, 0.4471],\n        [0.8141, 0.8319, 0.7213, 0.1050, 0.0814],\n        [0.3156, 0.3960, 0.2553, 0.8680, 0.9293]])\ntensor([[0.7019, 0.3004],\n        [0.8141, 0.8319],\n        [0.3156, 0.3960]])\ntensor([[0.6121, 0.8929],\n        [0.7213, 0.1050],\n        [0.2553, 0.8680]])\ntensor([[0.4471],\n        [0.0814],\n        [0.9293]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3,5)\n",
    "print(a)\n",
    "for i, data_i in enumerate(a.chunk(3, 1)): # 沿1轴分为3块\n",
    "    print(data_i)"
   ]
  },
  {
   "source": [
    "## 张量运算"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 矩阵乘法"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([2, 3])\ntorch.Size([3, 2])\ntorch.Size([2, 2])\ntorch.Size([2, 2, 3])\ntorch.Size([2, 3, 2])\ntorch.Size([2, 2, 2])\ntorch.Size([2, 3])\ntorch.Size([2, 3])\ntorch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.tensor([[0,1,2], [2,4,1]])\n",
    "tensor2 = torch.tensor([[0,1], [2,4], [1,2]])\n",
    "print(tensor1.shape)\n",
    "print(tensor2.shape)\n",
    "# Matrix multiplication: (m*n) * (n*p) -> (m*p).  torch.matmul类似，不过torch.mm针对二维矩阵，torch.matmul是高维。当torch.mm用于大于二维时将报错。\n",
    "result1 = torch.mm(tensor1, tensor2)\n",
    "print(result1.shape)\n",
    "\n",
    "tensor1 = torch.tensor([[[0,1,2], [2,4,1]], [[0,1,2], [2,4,1]]])\n",
    "tensor2 = torch.tensor([[[0,1], [2,4], [1,2]], [[0,1], [2,4], [1,2]]])\n",
    "print(tensor1.shape)\n",
    "print(tensor2.shape)\n",
    "# Batch matrix multiplication: (b*m*n) * (b*n*p) -> (b*m*p).\n",
    "result2 = torch.bmm(tensor1, tensor2)\n",
    "print(result2.shape)\n",
    "\n",
    "tensor1 = torch.tensor([[0,1,2], [2,4,1]])\n",
    "tensor2 = torch.tensor([[0,1,2], [2,4,1]])\n",
    "print(tensor1.shape)\n",
    "print(tensor2.shape)\n",
    "# Element-wise multiplication.  torch.mul类似\n",
    "# 当a, b维度不一致时，会自动填充到相同维度相点乘。\n",
    "result3 = tensor1 * tensor2\n",
    "print(result3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 4,  8],\n        [ 9, 20]])\n"
     ]
    }
   ],
   "source": [
    "# torch.einsum: 爱因斯坦求和\n",
    "tensor1 = torch.tensor([[0,1,2], [2,4,1]])\n",
    "tensor2 = torch.tensor([[0,1], [2,4], [1,2]])\n",
    "output = torch.einsum('ik, kj -> ij', tensor1, tensor2)\n",
    "# 'ik, kj -> ij'语义解释如下：\n",
    "# 输入tensor1: 2维数组，下标为ik,\n",
    "# 输入tensor2: 2维数组，下标为kj,\n",
    "# 输出output：2维数组，下标为ij。\n",
    "# 隐含语义：输入a,b下标中相同的k，是求和的下标\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.3191)\ntensor(-1.1421)\ntorch.return_types.slogdet(\nsign=tensor(1.),\nlogabsdet=tensor(-1.1421))\n"
     ]
    }
   ],
   "source": [
    "# 行列式计算\n",
    "A = torch.randn(3, 3)\n",
    "print(torch.det(A))\n",
    "print(torch.logdet(A))\n",
    "print(torch.slogdet(A)) # 返回一个元组，第一个元素是符号，第二个元素是行列式的对数绝对值"
   ]
  },
  {
   "source": [
    "### 计算两组数据之间的两两欧式距离"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[0, 1],\n         [0, 1]],\n\n        [[2, 4],\n         [2, 4]],\n\n        [[1, 2],\n         [1, 2]]])\ntensor([[[2, 4],\n         [1, 2]],\n\n        [[2, 4],\n         [1, 2]],\n\n        [[2, 4],\n         [1, 2]]])\n"
     ]
    }
   ],
   "source": [
    "# X1 is of shape m*d.\n",
    "X1 = torch.tensor([[0,1], [2,4], [1,2]])\n",
    "X1 = torch.unsqueeze(X1, dim=1).expand(3, 2, 2)\n",
    "print(X1)\n",
    "# X2 is of shape n*d.\n",
    "X2 = torch.tensor([[2,4], [1,2]])\n",
    "X2 = torch.unsqueeze(X2, dim=0).expand(3, 2, 2)\n",
    "print(X2)\n",
    "# dist is of shape m*n, where dist[i][j] = sqrt(|X1[i, :] - X[j, :]|^2)\n",
    "dist = torch.sqrt(torch.sum((X1 - X2) ** 2, dim=2))"
   ]
  },
  {
   "source": [
    "### element-wise运算"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "激活函数\n",
    "    torch.relu(x) # 修正线性单元，通常指代以斜坡函数及其变种为代表的非线性函数\n",
    "    torch.sigmoid(x) # 将变量映射到0,1之间\n",
    "    torch.tanh(x) # 图像为过原点并将变量映射到-1,1之间\n",
    "    F.softplus(x) # relu函数的平滑版本\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([-1., -0., -1.,  1.,  0.,  1.])\n"
     ]
    }
   ],
   "source": [
    "# 数学计算\n",
    "print(torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 3.0000,  7.0000],\n        [ 0.2321, -2.8660],\n        [-3.2321, -1.1340]])\n"
     ]
    }
   ],
   "source": [
    "# 傅立叶变换\n",
    "t = torch.tensor([[0,1], [2,4], [1,2]]).float()\n",
    "print(torch.fft(t,1)) # 参数signal_ndim只支持1、2、3；输入tensor至少为signal_ndim+1维"
   ]
  },
  {
   "source": [
    "### autograd"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(2.)\ntensor(1.)\ntensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Create tensors.\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "# Build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3\n",
    "\n",
    "# Compute gradients.\n",
    "y.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print(x.grad)    # x.grad = 2 \n",
    "print(w.grad)    # w.grad = 1 \n",
    "print(b.grad)    # b.grad = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "w:  Parameter containing:\ntensor([[ 0.3758,  0.0854, -0.2292],\n        [ 0.0418, -0.5129,  0.4411]], requires_grad=True)\nb:  Parameter containing:\ntensor([-0.0129, -0.2960], requires_grad=True)\nloss:  2.2790355682373047\ndL/dw:  tensor([[ 0.5613, -0.6708,  0.3478],\n        [-0.0215, -1.3205,  0.6146]])\ndL/db:  tensor([ 0.2287, -0.2668])\nloss after 1 step optimization:  2.2479350566864014\n"
     ]
    }
   ],
   "source": [
    "# Create tensors of shape (10, 3) and (10, 2).\n",
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)\n",
    "\n",
    "# Build a fully connected layer.\n",
    "linear = nn.Linear(3, 2)\n",
    "print ('w: ', linear.weight)\n",
    "print ('b: ', linear.bias)\n",
    "\n",
    "# Build loss function and optimizer.\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# Forward pass.\n",
    "pred = linear(x)\n",
    "\n",
    "# Compute loss.\n",
    "loss = criterion(pred, y)\n",
    "print('loss: ', loss.item())\n",
    "\n",
    "# Backward pass.\n",
    "loss.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)\n",
    "\n",
    "# 1-step gradient descent.\n",
    "optimizer.step()\n",
    "\n",
    "# You can also perform gradient descent at the low level.\n",
    "# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "\n",
    "# Print out the loss after 1-step gradient descent.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss after 1 step optimization: ', loss.item())"
   ]
  },
  {
   "source": [
    "## Pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### input"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e682246bf010424884f2b030abcff585"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data/\n",
      "torch.Size([3, 32, 32])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "# Download and construct CIFAR-10 dataset.\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='data/',\n",
    "                                             train=True, \n",
    "                                             transform=transforms.ToTensor(),\n",
    "                                             download=True)\n",
    "\n",
    "# Fetch one data pair (read data from disk).\n",
    "image, label = train_dataset[0]\n",
    "print (image.size())\n",
    "print (label)\n",
    "\n",
    "# Data loader (this provides queues and threads in a very simple way).\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)\n",
    "# 也可使用自定义数据集\n",
    "# custom_dataset = CustomDataset() # CustomDataset(torch.utils.data.Dataset)，需定义好__init__(self)、__getitem__(self, index)和__len__(self)\n",
    "\n",
    "# When iteration starts, queue and thread start to load data from files.\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "# Mini-batch images and labels.\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "# Actual usage of the data loader is as below.\n",
    "for images, labels in train_loader:\n",
    "    # Training code should be written here.\n",
    "    pass"
   ]
  },
  {
   "source": [
    "### 预训练模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /home/yaoqf/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=46827520.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c1cab8fb5d404880973f2085814c1a3d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "# Download and load the pretrained ResNet-18.\n",
    "resnet = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# If you want to finetune only the top layer of the model, set as below.\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the top layer for finetuning.\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 100)  # 100 is an example.\n",
    "\n",
    "# Forward pass.\n",
    "images = torch.randn(64, 3, 224, 224)\n",
    "outputs = resnet(images)\n",
    "print (outputs.size())     # (64, 100)"
   ]
  },
  {
   "source": [
    "### 存储和下载模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Save and load the entire model.\n",
    "torch.save(resnet, 'model.ckpt')\n",
    "model = torch.load('model.ckpt')\n",
    "\n",
    "# Save and load only the model parameters (recommended).\n",
    "torch.save(resnet.state_dict(), 'params.ckpt')\n",
    "resnet.load_state_dict(torch.load('params.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}