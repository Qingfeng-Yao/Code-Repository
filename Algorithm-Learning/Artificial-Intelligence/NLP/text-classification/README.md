## 文本分类(单标签)
- 传统文本分类：特征工程+分类算法，其中特征工程中用的最多的是词袋特征(稀疏词汇特征，如tfidf)
  - 词袋缺点：稀疏性、多义性、同义性、忽略不同词的语义相关性
  - 可与规则相结合(通过为那些没有被基本分类器正确建模的冲突标签添加特定规则)
- 基于深度学习的文本分类
  - 基于词嵌入的模型：无监督；non-contextual(不考虑词的上下文，为特定单词提供相同的向量)
    - Skip-gram：使用目标词的嵌入来预测上下文词的嵌入
    - Paragraph Vector：使用句子/文档嵌入来预测该句子/文档中词的嵌入
  - 使用深度学习模型如CNN/RNN作文本分类：需要训练数据(用语义相关性表示潜在特征)；需要更多的训练数据(至少需要数百万个标记的示例)
    - 可捕捉局部连续词序列中的语义和句法信息
    - 添加注意力机制
    - GNN：考虑全局词共现(不连续、距离远)
    - 传统机器学习算法如SVM和NB达到一定的阈值时增加更多的训练数据并不能提高其准确性，相比之下，深度学习分类器可继续获得更好的结果
- 预训练模型/fine-tune model
  - bert: contextualised vectors
- 无监督文本分类
  - 聚类
    - 基于稀疏词汇特征
    - 基于深度学习
  - 利用文档和类别标签之间的相似性
    - LSA+cos
    - vec+cos
- zero-shot文本分类：将合适的标签与一段文本相关联(无论文本的领域与标签所描述的方面)
  - 评估：标签部分可见-->标签全不可见

### CNN
- 两个通道：基于预训练词向量(word2vec)；学习特定任务向量
- 一维卷积、一层卷积
- 句子分类

### GCN
- 基于词共现和文档-词关系为一整个语料库建立一个异质文本图(节点为词和文档)
  - 词节点间的边由词共现信息确定，词节点和文档节点间的边由TF-IDF确定
      - 词共现/滑动窗口：PMI(正值表明词之间具有高语义关联，负值表明无关联)
- Text GCN(以Kipf GCN为基础): 没有使用预训练词嵌入或额外知识；利用词和文档的one-hot表示(特征矩阵为单位矩阵)初始化模型
  - 将文档分类转换成节点分类问题
  - 模型的健壮性：减少训练数据
  
### 无监督+文本相似性
- 动机来源于银行业，特别是经营operational风险的管理，这类风险对应于一系列既不是信用风险也不是市场风险的事件，包括与内部和外部欺诈、网络安全、有形资产损害、自然灾害等有关的问题。经营风险的实际管理部分基于对历史经营风险事件数据集的管理，其中每个事件都有详细描述，并定期与监管机构共享，映射到监管机构发布的大约20类风险中。为了进一步细致分到264类(每个标签都有几个词描述)中，则遇到没有训练集的挑战
- 利用文本相似性(LSA+cos)，通过定义良好的标签表示(反映其语义和词汇)(结合人类专家和语言模型)，计算文本与标签的相似性，进而确定文本的类别
  - 两个词集合：一个是文档中最相关的词，另一个是标签关键词
  - 标签关键词：
    - enrichment
      - Expert Knowledge：要求人类专家为每个标签提供3到5个附加单词
      - 获得同义词
      - 使用有代表性的文档：利用LSA确定(经验选取阈值70%)，并将所有有代表性的文档中的词加入标签关键词中
      - 使用词嵌入：进一步捕获语义上相似的词；使用预训练模型获得一般领域中的词，再获得特定领域中的词
    - consolidation
      - 减少标签关键词之间的重叠: FAC
- 可以与简单的有监督方法相媲美

### PTE
- 文本数据的半监督表示学习
  - 得到的低维嵌入不仅保留词和文档之间的语义紧密性，而且对特定任务还具有很强的预测能力
- 异质文本网络：编码不同级别的共现信息(包括词-词/词共现次数、词-文档/二部图/词频、词-标签/二部图/词频和，分属三个网络)
  - 集成有标签和无标签信息(词-词与词-文档编码无标签信息，词-标签编码有标签信息)
  - 二部图嵌入
    - 保留二阶proximity(LINE)
  - 异质文本网络嵌入：将三个二部图共同嵌入
    - 由三个二部图组成，且词节点在三个网络中共享
    - 首先学习词的向量表示
  
  
## 动态文本分类
- 标签变动(会增删)：只要增删metric space中的支持点(即文本)即可
- 解决思路：将固定大小的输出层替换成一个可学习的semantically meaningful metric space，优点是只需很小的训练集和利用了相似标签的信息
  - 学习一个嵌入函数(将输入文本映射到一个semantically meaningful metric space)

## 垃圾文本分类
- 二元分类
- 利用异常检测的思路

### 中文垃圾检测
- 解决不平衡、效率和文本伪装问题(集成激活学习和半监督生成学习 --> 文本垃圾检测)
  - 提出了一种“self-diversity”标准来衡量候选注释的“价值”
  - 数据增强：半监督变分自动编码机 + masked注意力学习；字符变异图增强扩充过程
  - 模型对垃圾样本选择敏感
  - 模型能够改善一系列传统激活学习模型(中文垃圾检测任务)的性能
- (pool-based)激活学习：针对少标签数据 --> 中文垃圾检测
  - 不平衡：垃圾样本和正常样本不平衡，使得激活学习模型会对垃圾样本更敏感
  - 效率：无标签样本巨大，经典的基于diversity的方法(迭代地将每个未标记的样本与每个已标记的样本进行比较，以选择最“多样化”的样本进行注释)效果差(由于计算复杂度为O(n^2))
  - 伪装：中文字符有字形和语音上的变异(故意对汉字进行变异以逃避垃圾邮件检测的算法)


## 参考文献
- 2014 | EMNLP | Convolutional Neural Networks for Sentence Classification | Yoon Kim
- 2015 ｜ KDD ｜ PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks ｜ Jian Tang et al.
- 2019 | AAAI | Graph Convolutional Networks for Text Classification | Liang Yao et al.
- 2019 | ACL | Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings ｜ Zied Haj-Yahia et al.

