## 任务
- 语言建模

## 数据集
- PTB(Penn Treebank): character/word
- enwik8(Hutter Prize dataset): character
- WikiText-2(WT2): word
- WikiText-103(WT103): word

## 模型
- RNN：LSTM、GRU、QRNN
- Transformer
- 流模型

## 指标
- NLL(负对数似然)，单位bpc/bpw，即bits per character/bits per word


## 特点
- 提升GPU利用率



## 参考资料
- [salesforce/awd-lstm-lm](https://github.com/salesforce/awd-lstm-lm)
- [harvardnlp/TextFlow](https://github.com/harvardnlp/TextFlow)
- [TrentBrick/PyTorchDiscreteFlows](https://github.com/TrentBrick/PyTorchDiscreteFlows)
- [nadavbh12/Character-Level-Language-Modeling-with-Deeper-Self-Attention-pytorch](https://github.com/nadavbh12/Character-Level-Language-Modeling-with-Deeper-Self-Attention-pytorch)